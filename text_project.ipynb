{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Wednesday December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory, but if you use the keras.embedding layer, it will be more efficient. \n",
    "- Loading all documents into one big matrix as we have done in the exercises is not feasible (e.g. the virtual servers in CSC have only 3 GB of RAM). You need to load the documents in smaller chunks for the training. This shouldn't be a problem, as we are doing mini-batch training anyway, and thus we don't need to keep all the documents in memory. You can simply pass you current chunk of documents to `model.fit()` as it remembers the weights from the previous run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/DJKesoil/Opiskelu/deep/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set already downloaded.\n",
      "\n",
      "Data set already unzipped.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import os\n",
    "import zipfile\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "database_path = 'train/'\n",
    "corpus_path = database_path + 'REUTERS_CORPUS_2/'\n",
    "data_path = corpus_path + 'data/'\n",
    "codes_path = corpus_path + 'codes/'\n",
    "\n",
    "if not os.path.exists(database_path):\n",
    "    dl_file='reuters.zip'\n",
    "    dl_url='https://www.cs.helsinki.fi/u/jgpyykko/'\n",
    "    get_file(dl_file, dl_url+dl_file, cache_dir='./', cache_subdir=database_path, extract=True)\n",
    "else:\n",
    "    print('Data set already downloaded.')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print('\\n\\nUnzipping data...')\n",
    "    \n",
    "    codes_zip = corpus_path + 'codes.zip'\n",
    "    with zipfile.ZipFile(codes_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(codes_path)\n",
    "    os.remove(codes_zip)\n",
    "   \n",
    "    dtds_zip = corpus_path + 'dtds.zip'\n",
    "    with zipfile.ZipFile(dtds_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(corpus_path + 'dtds/')\n",
    "    os.remove(dtds_zip)\n",
    "    \n",
    "    for item in os.listdir(corpus_path): \n",
    "        if item.endswith('zip'):\n",
    "            file_name = corpus_path + item \n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(data_path)\n",
    "            os.remove(file_name) \n",
    "    \n",
    "    print('Data set unzipped.')\n",
    "else:\n",
    "    print('\\nData set already unzipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "First we will read the codes into the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126  different classes\n",
      "\n",
      "\n",
      "['1POL', '2ECO', '3SPO', '4GEN', '6INS', '7RSK', '8YDB', '9BNX', 'ADS10', 'BNW14', 'BRP11', 'C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16', 'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182', 'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313', 'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11', 'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142', 'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313', 'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT', 'ENT12', 'G11', 'G111', 'G112', 'G113', 'G12', 'G13', 'G131', 'G14', 'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157', 'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GEDU', 'GENT', 'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL', 'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA', 'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141', 'M142', 'M143', 'MCAT', 'MEUR', 'PRB13'] \n",
      "\n",
      "\n",
      "{'1POL': 0, '2ECO': 1, '3SPO': 2, '4GEN': 3, '6INS': 4, '7RSK': 5, '8YDB': 6, '9BNX': 7, 'ADS10': 8, 'BNW14': 9, 'BRP11': 10, 'C11': 11, 'C12': 12, 'C13': 13, 'C14': 14, 'C15': 15, 'C151': 16, 'C1511': 17, 'C152': 18, 'C16': 19, 'C17': 20, 'C171': 21, 'C172': 22, 'C173': 23, 'C174': 24, 'C18': 25, 'C181': 26, 'C182': 27, 'C183': 28, 'C21': 29, 'C22': 30, 'C23': 31, 'C24': 32, 'C31': 33, 'C311': 34, 'C312': 35, 'C313': 36, 'C32': 37, 'C33': 38, 'C331': 39, 'C34': 40, 'C41': 41, 'C411': 42, 'C42': 43, 'CCAT': 44, 'E11': 45, 'E12': 46, 'E121': 47, 'E13': 48, 'E131': 49, 'E132': 50, 'E14': 51, 'E141': 52, 'E142': 53, 'E143': 54, 'E21': 55, 'E211': 56, 'E212': 57, 'E31': 58, 'E311': 59, 'E312': 60, 'E313': 61, 'E41': 62, 'E411': 63, 'E51': 64, 'E511': 65, 'E512': 66, 'E513': 67, 'E61': 68, 'E71': 69, 'ECAT': 70, 'ENT12': 71, 'G11': 72, 'G111': 73, 'G112': 74, 'G113': 75, 'G12': 76, 'G13': 77, 'G131': 78, 'G14': 79, 'G15': 80, 'G151': 81, 'G152': 82, 'G153': 83, 'G154': 84, 'G155': 85, 'G156': 86, 'G157': 87, 'G158': 88, 'G159': 89, 'GCAT': 90, 'GCRIM': 91, 'GDEF': 92, 'GDIP': 93, 'GDIS': 94, 'GEDU': 95, 'GENT': 96, 'GENV': 97, 'GFAS': 98, 'GHEA': 99, 'GJOB': 100, 'GMIL': 101, 'GOBIT': 102, 'GODD': 103, 'GPOL': 104, 'GPRO': 105, 'GREL': 106, 'GSCI': 107, 'GSPO': 108, 'GTOUR': 109, 'GVIO': 110, 'GVOTE': 111, 'GWEA': 112, 'GWELF': 113, 'M11': 114, 'M12': 115, 'M13': 116, 'M131': 117, 'M132': 118, 'M14': 119, 'M141': 120, 'M142': 121, 'M143': 122, 'MCAT': 123, 'MEUR': 124, 'PRB13': 125} \n",
      "\n",
      "\n",
      "1POL  :  CURRENT NEWS - POLITICS\n",
      "2ECO  :  CURRENT NEWS - ECONOMICS\n",
      "3SPO  :  CURRENT NEWS - SPORT\n",
      "4GEN  :  CURRENT NEWS - GENERAL\n",
      "6INS  :  CURRENT NEWS - INSURANCE\n",
      "7RSK  :  CURRENT NEWS - RISK NEWS\n",
      "8YDB  :  TEMPORARY\n",
      "9BNX  :  TEMPORARY\n",
      "ADS10  :  CURRENT NEWS - ADVERTISING\n",
      "BNW14  :  CURRENT NEWS - BUSINESS NEWS\n",
      "BRP11  :  CURRENT NEWS - BRANDS\n",
      "C11  :  STRATEGY/PLANS\n",
      "C12  :  LEGAL/JUDICIAL\n",
      "C13  :  REGULATION/POLICY\n",
      "C14  :  SHARE LISTINGS\n",
      "C15  :  PERFORMANCE\n",
      "C151  :  ACCOUNTS/EARNINGS\n",
      "C1511  :  ANNUAL RESULTS\n",
      "C152  :  COMMENT/FORECASTS\n",
      "C16  :  INSOLVENCY/LIQUIDITY\n",
      "C17  :  FUNDING/CAPITAL\n",
      "C171  :  SHARE CAPITAL\n",
      "C172  :  BONDS/DEBT ISSUES\n",
      "C173  :  LOANS/CREDITS\n",
      "C174  :  CREDIT RATINGS\n",
      "C18  :  OWNERSHIP CHANGES\n",
      "C181  :  MERGERS/ACQUISITIONS\n",
      "C182  :  ASSET TRANSFERS\n",
      "C183  :  PRIVATISATIONS\n",
      "C21  :  PRODUCTION/SERVICES\n",
      "C22  :  NEW PRODUCTS/SERVICES\n",
      "C23  :  RESEARCH/DEVELOPMENT\n",
      "C24  :  CAPACITY/FACILITIES\n",
      "C31  :  MARKETS/MARKETING\n",
      "C311  :  DOMESTIC MARKETS\n",
      "C312  :  EXTERNAL MARKETS\n",
      "C313  :  MARKET SHARE\n",
      "C32  :  ADVERTISING/PROMOTION\n",
      "C33  :  CONTRACTS/ORDERS\n",
      "C331  :  DEFENCE CONTRACTS\n",
      "C34  :  MONOPOLIES/COMPETITION\n",
      "C41  :  MANAGEMENT\n",
      "C411  :  MANAGEMENT MOVES\n",
      "C42  :  LABOUR\n",
      "CCAT  :  CORPORATE/INDUSTRIAL\n",
      "E11  :  ECONOMIC PERFORMANCE\n",
      "E12  :  MONETARY/ECONOMIC\n",
      "E121  :  MONEY SUPPLY\n",
      "E13  :  INFLATION/PRICES\n",
      "E131  :  CONSUMER PRICES\n",
      "E132  :  WHOLESALE PRICES\n",
      "E14  :  CONSUMER FINANCE\n",
      "E141  :  PERSONAL INCOME\n",
      "E142  :  CONSUMER CREDIT\n",
      "E143  :  RETAIL SALES\n",
      "E21  :  GOVERNMENT FINANCE\n",
      "E211  :  EXPENDITURE/REVENUE\n",
      "E212  :  GOVERNMENT BORROWING\n",
      "E31  :  OUTPUT/CAPACITY\n",
      "E311  :  INDUSTRIAL PRODUCTION\n",
      "E312  :  CAPACITY UTILIZATION\n",
      "E313  :  INVENTORIES\n",
      "E41  :  EMPLOYMENT/LABOUR\n",
      "E411  :  UNEMPLOYMENT\n",
      "E51  :  TRADE/RESERVES\n",
      "E511  :  BALANCE OF PAYMENTS\n",
      "E512  :  MERCHANDISE TRADE\n",
      "E513  :  RESERVES\n",
      "E61  :  HOUSING STARTS\n",
      "E71  :  LEADING INDICATORS\n",
      "ECAT  :  ECONOMICS\n",
      "ENT12  :  CURRENT NEWS - ENTERTAINMENT\n",
      "G11  :  SOCIAL AFFAIRS\n",
      "G111  :  HEALTH/SAFETY\n",
      "G112  :  SOCIAL SECURITY\n",
      "G113  :  EDUCATION/RESEARCH\n",
      "G12  :  INTERNAL POLITICS\n",
      "G13  :  INTERNATIONAL RELATIONS\n",
      "G131  :  DEFENCE\n",
      "G14  :  ENVIRONMENT\n",
      "G15  :  EUROPEAN COMMUNITY\n",
      "G151  :  EC INTERNAL MARKET\n",
      "G152  :  EC CORPORATE POLICY\n",
      "G153  :  EC AGRICULTURE POLICY\n",
      "G154  :  EC MONETARY/ECONOMIC\n",
      "G155  :  EC INSTITUTIONS\n",
      "G156  :  EC ENVIRONMENT ISSUES\n",
      "G157  :  EC COMPETITION/SUBSIDY\n",
      "G158  :  EC EXTERNAL RELATIONS\n",
      "G159  :  EC GENERAL\n",
      "GCAT  :  GOVERNMENT/SOCIAL\n",
      "GCRIM  :  CRIME, LAW ENFORCEMENT\n",
      "GDEF  :  DEFENCE\n",
      "GDIP  :  INTERNATIONAL RELATIONS\n",
      "GDIS  :  DISASTERS AND ACCIDENTS\n",
      "GEDU  :  EDUCATION\n",
      "GENT  :  ARTS, CULTURE, ENTERTAINMENT\n",
      "GENV  :  ENVIRONMENT AND NATURAL WORLD\n",
      "GFAS  :  FASHION\n",
      "GHEA  :  HEALTH\n",
      "GJOB  :  LABOUR ISSUES\n",
      "GMIL  :  MILLENNIUM ISSUES\n",
      "GOBIT  :  OBITUARIES\n",
      "GODD  :  HUMAN INTEREST\n",
      "GPOL  :  DOMESTIC POLITICS\n",
      "GPRO  :  BIOGRAPHIES, PERSONALITIES, PEOPLE\n",
      "GREL  :  RELIGION\n",
      "GSCI  :  SCIENCE AND TECHNOLOGY\n",
      "GSPO  :  SPORTS\n",
      "GTOUR  :  TRAVEL AND TOURISM\n",
      "GVIO  :  WAR, CIVIL WAR\n",
      "GVOTE  :  ELECTIONS\n",
      "GWEA  :  WEATHER\n",
      "GWELF  :  WELFARE, SOCIAL SERVICES\n",
      "M11  :  EQUITY MARKETS\n",
      "M12  :  BOND MARKETS\n",
      "M13  :  MONEY MARKETS\n",
      "M131  :  INTERBANK MARKETS\n",
      "M132  :  FOREX MARKETS\n",
      "M14  :  COMMODITY MARKETS\n",
      "M141  :  SOFT COMMODITIES\n",
      "M142  :  METALS TRADING\n",
      "M143  :  ENERGY MARKETS\n",
      "MCAT  :  MARKETS\n",
      "MEUR  :  EURO CURRENCY\n",
      "PRB13  :  CURRENT NEWS - PRESS RELEASE WIRES\n",
      "1POL  :  0\n",
      "2ECO  :  1\n",
      "3SPO  :  2\n",
      "4GEN  :  3\n",
      "6INS  :  4\n",
      "7RSK  :  5\n",
      "8YDB  :  6\n",
      "9BNX  :  7\n",
      "ADS10  :  8\n",
      "BNW14  :  9\n",
      "BRP11  :  10\n",
      "C11  :  11\n",
      "C12  :  12\n",
      "C13  :  13\n",
      "C14  :  14\n",
      "C15  :  15\n",
      "C151  :  16\n",
      "C1511  :  17\n",
      "C152  :  18\n",
      "C16  :  19\n",
      "C17  :  20\n",
      "C171  :  21\n",
      "C172  :  22\n",
      "C173  :  23\n",
      "C174  :  24\n",
      "C18  :  25\n",
      "C181  :  26\n",
      "C182  :  27\n",
      "C183  :  28\n",
      "C21  :  29\n",
      "C22  :  30\n",
      "C23  :  31\n",
      "C24  :  32\n",
      "C31  :  33\n",
      "C311  :  34\n",
      "C312  :  35\n",
      "C313  :  36\n",
      "C32  :  37\n",
      "C33  :  38\n",
      "C331  :  39\n",
      "C34  :  40\n",
      "C41  :  41\n",
      "C411  :  42\n",
      "C42  :  43\n",
      "CCAT  :  44\n",
      "E11  :  45\n",
      "E12  :  46\n",
      "E121  :  47\n",
      "E13  :  48\n",
      "E131  :  49\n",
      "E132  :  50\n",
      "E14  :  51\n",
      "E141  :  52\n",
      "E142  :  53\n",
      "E143  :  54\n",
      "E21  :  55\n",
      "E211  :  56\n",
      "E212  :  57\n",
      "E31  :  58\n",
      "E311  :  59\n",
      "E312  :  60\n",
      "E313  :  61\n",
      "E41  :  62\n",
      "E411  :  63\n",
      "E51  :  64\n",
      "E511  :  65\n",
      "E512  :  66\n",
      "E513  :  67\n",
      "E61  :  68\n",
      "E71  :  69\n",
      "ECAT  :  70\n",
      "ENT12  :  71\n",
      "G11  :  72\n",
      "G111  :  73\n",
      "G112  :  74\n",
      "G113  :  75\n",
      "G12  :  76\n",
      "G13  :  77\n",
      "G131  :  78\n",
      "G14  :  79\n",
      "G15  :  80\n",
      "G151  :  81\n",
      "G152  :  82\n",
      "G153  :  83\n",
      "G154  :  84\n",
      "G155  :  85\n",
      "G156  :  86\n",
      "G157  :  87\n",
      "G158  :  88\n",
      "G159  :  89\n",
      "GCAT  :  90\n",
      "GCRIM  :  91\n",
      "GDEF  :  92\n",
      "GDIP  :  93\n",
      "GDIS  :  94\n",
      "GEDU  :  95\n",
      "GENT  :  96\n",
      "GENV  :  97\n",
      "GFAS  :  98\n",
      "GHEA  :  99\n",
      "GJOB  :  100\n",
      "GMIL  :  101\n",
      "GOBIT  :  102\n",
      "GODD  :  103\n",
      "GPOL  :  104\n",
      "GPRO  :  105\n",
      "GREL  :  106\n",
      "GSCI  :  107\n",
      "GSPO  :  108\n",
      "GTOUR  :  109\n",
      "GVIO  :  110\n",
      "GVOTE  :  111\n",
      "GWEA  :  112\n",
      "GWELF  :  113\n",
      "M11  :  114\n",
      "M12  :  115\n",
      "M13  :  116\n",
      "M131  :  117\n",
      "M132  :  118\n",
      "M14  :  119\n",
      "M141  :  120\n",
      "M142  :  121\n",
      "M143  :  122\n",
      "MCAT  :  123\n",
      "MEUR  :  124\n",
      "PRB13  :  125\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "topic_labels = {}\n",
    "codes_file = codes_path + 'topic_codes.txt'\n",
    "with open(codes_file) as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(';'):\n",
    "            splits = line.split()\n",
    "            topic_code = splits[0]\n",
    "            topic_labels[topic_code] = ' '.join(splits[1:len(splits)])\n",
    "            topics.append(topic_code)\n",
    "\n",
    "n_class = len(topics)\n",
    "topic_index = {topics[i] : i for i in range(n_class)}\n",
    "\n",
    "\n",
    "print(n_class, ' different classes\\n\\n')\n",
    "print(topics, '\\n\\n')\n",
    "print(topic_index, '\\n\\n')\n",
    "\n",
    "for key in topic_labels:\n",
    "    print(key, ' : ', topic_labels[key])\n",
    "\n",
    "for key in topic_index:\n",
    "    print(key, ' : ', topic_index[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will parse the xml files. Let's first try to parse one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tags:  ['GCAT', 'GVIO'] \n",
      "\n",
      "\n",
      "sentences:  ['Sri Lanka rebels attack government troops in north.', \"Heavy fighting erupted between government troops and Liberation Tigers of Tamil Eelam (LTTE) rebels in Sri Lanka's northern Wanni region late on Friday, military officials said on Saturday.\", \"They said the rebels had attacked the military's defensive positions just north of the government-held town of Vavuniya, some 220 km (135 miles) north of the capital Colombo.\", 'The military, which launched a major offensive in May, is battling rebels in the Wanni to open a strategic highway linking the northern Jaffna peninsula with the rest of the island.', \"The military officials said defences guarded by police and the navy had been breached but troops had linked up again after repulsing Friday's attack.\", 'Casualty figures and other details were not immediately available, but officials said troops were clearing the area.', 'An undisclosed number of wounded had been airlifted to Anuradhapura military base, some 50 km (31 miles) south of Vavuniya, officials said.', 'A Defence Ministry spokesman said in Colombo confirmed the attacks and said the situation was now under control.', 'Residents in Vavuniya said troops were shelling the affected area on Saturday morning.', '\"Shelling is still continuing and helicopters and air force planes are flying by,\" a resident said by phone from Vavuniya.', 'He said the fighting started at about 10.30 p.m. (1630 GMT Friday) and continued through most of the night.', \"The military's northward offensive has slowed since the LTTE launched two fierce counterattacks in June but the fighting has shown signs of escalating this week.\", \"Sri Lanka's Defence Ministry said on Friday that troops had clashed with a large number of LTTE rebels southwest of Nedunkeni town on Thursday.\", 'At least 50 guerrillas and 17 troops, including one officer, was killed in the gunbattle. Troops had called in helicopter gunships to engage the rebels, who fled northwards with their wounded, the statement said.', '\"Sri Lanka forces backed by heavy artillery and moving in battle-tanks and armoured vehicles had attempted a sudden push towards their goal of Puliyankulam, but LTTE forces hit back decisively,\" an LTTE statement said.', \"The LTTE have been fighting for a separate homeland for minority Tamils in Sri Lanka's north and east since 1983.\", 'The government says more than 50,000 people have been killed in the war. The LTTE say the toll is higher.'] \n",
      "\n",
      "\n",
      "Sri Lanka rebels attack government troops in north. \n",
      "\n",
      "Heavy fighting erupted between government troops and Liberation Tigers of Tamil Eelam (LTTE) rebels in Sri Lanka's northern Wanni region late on Friday, military officials said on Saturday. \n",
      "\n",
      "They said the rebels had attacked the military's defensive positions just north of the government-held town of Vavuniya, some 220 km (135 miles) north of the capital Colombo. \n",
      "\n",
      "The military, which launched a major offensive in May, is battling rebels in the Wanni to open a strategic highway linking the northern Jaffna peninsula with the rest of the island. \n",
      "\n",
      "The military officials said defences guarded by police and the navy had been breached but troops had linked up again after repulsing Friday's attack. \n",
      "\n",
      "Casualty figures and other details were not immediately available, but officials said troops were clearing the area. \n",
      "\n",
      "An undisclosed number of wounded had been airlifted to Anuradhapura military base, some 50 km (31 miles) south of Vavuniya, officials said. \n",
      "\n",
      "A Defence Ministry spokesman said in Colombo confirmed the attacks and said the situation was now under control. \n",
      "\n",
      "Residents in Vavuniya said troops were shelling the affected area on Saturday morning. \n",
      "\n",
      "\"Shelling is still continuing and helicopters and air force planes are flying by,\" a resident said by phone from Vavuniya. \n",
      "\n",
      "He said the fighting started at about 10.30 p.m. (1630 GMT Friday) and continued through most of the night. \n",
      "\n",
      "The military's northward offensive has slowed since the LTTE launched two fierce counterattacks in June but the fighting has shown signs of escalating this week. \n",
      "\n",
      "Sri Lanka's Defence Ministry said on Friday that troops had clashed with a large number of LTTE rebels southwest of Nedunkeni town on Thursday. \n",
      "\n",
      "At least 50 guerrillas and 17 troops, including one officer, was killed in the gunbattle. Troops had called in helicopter gunships to engage the rebels, who fled northwards with their wounded, the statement said. \n",
      "\n",
      "\"Sri Lanka forces backed by heavy artillery and moving in battle-tanks and armoured vehicles had attempted a sudden push towards their goal of Puliyankulam, but LTTE forces hit back decisively,\" an LTTE statement said. \n",
      "\n",
      "The LTTE have been fighting for a separate homeland for minority Tamils in Sri Lanka's north and east since 1983. \n",
      "\n",
      "The government says more than 50,000 people have been killed in the war. The LTTE say the toll is higher. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "filename_xml = '810900newsML.xml'\n",
    "file_xml = data_path + filename_xml\n",
    "\n",
    "def read_xml_file(file_xml):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    read_tags = False\n",
    "    for event, elem in etree.iterparse(file_xml, events=('start', 'end')):\n",
    "        t = elem.tag\n",
    "        idx = k = t.rfind(\"}\")\n",
    "        if idx != -1:\n",
    "            t = t[idx + 1:]\n",
    "        tname = t\n",
    "\n",
    "        if event == 'start':\n",
    "            if tname == 'codes':\n",
    "                if elem.attrib['class'] == 'bip:topics:1.0':\n",
    "                    read_tags = True\n",
    "            if tname == 'code':\n",
    "                if read_tags:\n",
    "                    tags.append(elem.attrib['code'])\n",
    "    \n",
    "        if event == 'end':\n",
    "            if tname == 'headline':\n",
    "                sentences.append(elem.text)\n",
    "            if tname == 'p':\n",
    "                sentences.append(elem.text)\n",
    "            if tname == 'codes':\n",
    "                if elem.attrib['class'] == 'bip:topics:1.0':\n",
    "                    read_tags = False\n",
    "\n",
    "    return [sentences, tags]\n",
    "    \n",
    "(sentences, tags) = read_xml_file(file_xml)\n",
    "\n",
    "print('\\n\\ntags: ', tags, '\\n\\n')\n",
    "print('sentences: ', sentences, '\\n\\n')\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read a small training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['M14', 'M141', 'MCAT'], ['C18', 'C181', 'CCAT'], ['GCAT', 'GCRIM', 'GDIP', 'GVIO']] \n",
      "\n",
      "['Australia says 1997/98 oat prices depend on rain.', 'The Australian Barley Board (ABB) said on Tuesday the Australian domestic market could be expected to have a major effect on oat prices in 1997/98, but just how much would depend on rainfall in spring.', '\"Our pool forecasts have been increased to A$140 per tonne for milling oats (in Victoria, oats number 1), and A$130 for feed oats, although these are still based on export potential,\" ABB chief executive Michael Iwaniw said in a statement.  ', 'Iwaniw said there were many negative factors on oats in the current world market, with a considerable production boost in the United States expected to offset slightly lower oat production in Canada and Sweden.', 'Domestic oat prices were high at present along with all grains in demand for the intensive stockfeed industries, particularly in Victoria, and demand was also evident from the grazing industry for feed grain due to the dry season, he said.  ', '\"If there is not substantial spring rainfall, the likelihood of significant oat exports will be reduced and our oat prices will see more of a domestic focus, which would see pool estimates rise accordingly,\" Iwaniw said.', 'The ABB said deliveries into the standard pool for 1996/97 oats would receive a payment on September 2 of A$12 per tonne, with A$10 per tonne payment in South Australia on feed oats 2 and 3. Deliveries into the 40/40/20 pool would receive A$56 per tonne for milling oats (oats number 1), A$51 for feed oats, and in South Australia A$34 for feed 2 and A$29 for feed 3.', '-- Sydney Newsroom 61-2 9373-1800'] \n",
      "\n",
      "['MDU RESOURCES BUYS HAWAIIAN CEMENT STAKE.', 'MDU Resources Group Inc said Thursday its Knife River Corp unit has purchased from Adelaide Brighton Cement Inc the remaining 50 percent general partnership interest in Hawaiian Cement that it did not own .', 'Financial terms of the deal were not disclosed.', 'Hawaiian Cement, a construction materials supplier, had revenues of $70 million in 1996, MDU said.', 'MDU shares were up 1/16 at 25-1/16.', 'Knife river is the construction materials and mining subsidiary of MDU Resources, which also has oil and natural gas acquisition, exploration and production activities, an electric and natural gas utility and a natural gas transmission company.', '((--Chicago newsdesk, 312-408-8787))'] \n",
      "\n",
      "['Former British POW vows to fight court battle.', \"A British veteran vowed on Monday to fight a court battle against the Japanese government to seek a formal apology and cash compensation for Japan's treatment of prisoners during World War Two.\", '\"I surrendered once to the Japanese. I didn\\'t like it,\" Arthur Titherington, secretary of the Japanese Labour Camp Survivors\\' Association, said.', '\"I will not surrender again,\" he told a news conference in Tokyo.', 'Titherington was representing some 10,000 British prisoners of war at a Tokyo district court.', 'The former POWs are demanding that the Japanese government make an unequivocal apology and pay each $22,000 in cash compensation.', '\"There is no hatred. There is no question of revenge. It is simply a matter of justice,\" the 75-year-old veteran said.', 'Titherington accused the Japanese government of \"sitting back and waiting for a natural solution to the problem\".', '\"The natural solution being the death of people like me,\" he said.', 'More than 12,000 Britons died from disease and starvation in Japanese war camps or in work gangs.', '\"Because of the procrastination of your government, you the ordinary Japanese are invariably refered to, certainly in Britain, with derogatory names, usually the bloody Japanese,\" Titherington said.', '\"Very seldom are you referred to as the Japanese,\" he said.', '\"You are held responsible for something you didn\\'t do,\" he said.'] \n",
      "\n",
      "[['GCAT', 'GSPO'], ['E21', 'E212', 'ECAT'], ['GCAT', 'GVIO']] \n",
      "\n",
      "['RUGBY UNION-Hurricanes blow away Queensland.', 'BRISBANE, April 14 - The Wellington Hurricanes passed a test of character not totally confined to rugby when they beat Queensland 47-29 in their Super 12 rugby match in Brisbane on Saturday night.', 'In the days leading up to the match, the Hurricanes had to deal with the emotional and mental wreckage associated with the rape charge levelled against player Roger Randle in South Africa.', \"The squad were in the Durban courtroom where Randle appeared for a pre-trial hearing after being charged with the rape of a woman in the team's hotel.  \", 'They then made the arduous trip from South Africa to Perth, then to Sydney -- where Randle left them to return home -- and on to Brisbane.', 'Jet-lagged and with many of the team unable to sleep at night, the Hurricanes had one of the worst preparations imaginable.', 'After 12 minutes of the game it was reflected in the score -- down 0-17 -- before captain Mark Allen launched them on a defiant fightback.  ', \"Afterwards, Wellington wouldn't talk directly about Randle and the effect his ordeal had on the team, but coach Frank Oliver admitted his side had been sorely tested by all that had befallen them in the previous week.\", '\"The challenge to any team when you\\'re up against it, when the odds are stacked against you, is to show heart, character and courage and this team has plenty of that,\" Oliver told NZPA after the match.', \"However, he said the challenge now for the team was to maintain the form they'd found on their short tour of South Africa and Brisbane.  \", '\"This win is a great present to take home after all we\\'ve been through, but we\\'ve got to keep going now.', '\"This win has given us the springboard. It\\'s lifted us out of the middle of the pack and near the leaders, now we have to maintain it and keep winning at home.\"', 'Allen inspired his team to come back from their disastrous start, saying the Hurricanes were on the point of capitulation at one stage.', '\"If they\\'d scored again after getting ahead 17-0 we would have been hurting,\" Allen said.  ', '\"We were already teetering at that point and another try would have broken us.', '\"But this team\\'s got a lot of character and they dug really deep to start going forward and get back into the game.\"', \"Allen wouldn't say so himself, but the Hurricanes' revival rested almost solely on his broad shoulders. When they were down, it was Allen who showed the way with some characteristic surges.\", 'It was his charge in the 16th minute that set the platform for the backline to get their first try -- a double skip pass for Tana Umaga to score with a strong finish.  ', \"Umaga made the burst that allowed Steven Bachop a free run to the line for the Hurricanes' second try four minutes later.\", 'And it was Allen again, bursting down field from a maul, who sparked a try for Alex Telea two minutes before halftime for an important 25-17 lead at the break.', 'Queensland stayed in the contest in the second half, thanks to a string of penalties conceded by Wellington, but the game ebbed away from them again midway through the spell when the powerful Hurricanes pack shoved them off the ball on their own scrum and two phases later had a try to Telea.  ', \"The stuffing seemed to fall out of Queensland after that with Jason O'Halloran busting through some weak defence to score under the crossbar and Jon Preston scoring a bizarre try after pouncing on his own kick after a penalty attempt hit the post.\", \"Wellington: 47 (A Telea 2, T Umaga, S Bachop, J O'Halloran, J Preston tries; J Preston 4 cons, 3 pens).\", 'Queensland: 29 (T Horan, W Carne tries; J Eales 2 cons, 5 pens). Halftime: 25-17.'] \n",
      "\n",
      "['Maryland $18 mln health agency bonds restructured.', \"Legg Mason Wood Walker, Inc., said Wednesday it restructured Maryland Health and Higher Educational Facilities Authority's $18 million bond deal, shifting the 2016 maturity to 2015.\", 'The restructuring follows:', 'MARYLAND HEALTH AND HIGHER EDUCATIONAL', 'FACILITIES AUTHORITY', 'RE: $18,305,000*', 'REFUNDING REVENUE BONDS', 'PICKERSGILL ISSUE', 'VERBAL AWARD.', 'NOTE RESTUCTURING.', '\\t\\t\\t\\t\\t    14,380,000*', '\\t\\t\\t\\t\\t    SERIES 1997A', \"    MOODY'S: NR\\t\\t  S&P: A-\", '    DATED: 01/01/1997\\t  FIRST COUPON: 07/01/1997', 'DUE: 01/01', 'INTEREST ACCRUES: 04/01/1997', ' MATURITY\\t\\tAMOUNT*    COUPON\\t   PRICE', '01/01/1999\\t\\t  415M    4.55  %\\t100.00', '01/01/2000\\t\\t  435M    4.80  %\\t100.00', '01/01/2001\\t\\t  460M    5.05  %\\t100.00', '01/01/2002\\t\\t  475M    5.15  %\\t100.00', '01/01/2003\\t\\t  505M    5.25  %\\t100.00', '--------------------------------------------------------------', '01/01/2004\\t\\t  535M    5.35  %\\t100.00', '01/01/2005\\t\\t  560M    5.45  %\\t100.00', '01/01/2006\\t\\t  585M    5.55  %\\t100.00', '01/01/2007\\t\\t  625M    5.65  %\\t100.00', '01/01/2008\\t\\t  655M    5.75  %\\t100.00', '--------------------------------------------------------------', '01/01/2009\\t\\t  695M    5.80  %\\t100.00', '01/01/2010\\t\\t  730M    5.85  %\\t100.00', '01/01/2015\\t\\t4,390M    6.00  %\\t  6.05', '\\t\\t\\t\\t\\t\\t  (Approx. $ Price 99.45 )', '01/01/2018\\t\\t3,315M    6.00  %\\t  6.10', '\\t\\t\\t\\t\\t\\t  (Approx. $ Price 98.822)', 'CALL FEATURES: 01/01/2007    102.00', '\\t\\t   DTP 01/01/2009', '----------------------------------------------------------------', '\\t\\t\\t\\t\\t     3,925,000*', '\\t\\t\\t\\t\\t   1997 SERIES B', '\\t\\t\\t\\t   ADJUSTABLE RATE SECURITIES', '\\t\\t\\t\\t\\t   1997 SERIES B', \"    MOODY'S: NR\\t\\t  S&P: A-\", '    DATED: 01/01/1997\\t  FIRST COUPON: 07/01/1997', 'DUE: 01/01', 'INTEREST ACCRUES: 04/01/1997', 'INITIAL RESET RATE:  1/1/02', 'ALL BONDS ARE PRICED AT PAR.', 'MATURITY\\t\\t DATE\\t    AMOUNT*    COUPON', '01/01/2021\\t\\t\\t\\t 3,925M     5.50  %', 'CALL FEATURES:  BONDS ARE NON-CALLABLE FOR THE FIRST RESET', 'PERIOD.', '** OPTIONAL CALL FEATURES THEREAFTER WILL VARY WITH TERMS OF', 'RESET PERIOED,  PLEASE REFER TO FINAL OFFICIAL STATEMENT.', '* - APPROXIMATE SUBJECT TO CHANGE', 'The compliance addendum MSRB Rule G-11 will apply.', 'The award is expected Thursday, April 3, 1997.', 'Delivery is expected on April 22, 1997.', 'This issue is book entry through DTC.', 'Legg Mason Wood Walker, Inc.', '--U.S. Municipal Desk, 212-859-1650'] \n",
      "\n",
      "['Gunmen fire at Russian car in Grozny, agency says.', 'Masked gunmen opened fire on a Russian government car in the capital of separatist-minded Chechnya on Sunday and tried to stop the vehicle, Interfax news agency reported.', \"It quoted General Sergei Sidorenko, the Russian Interior Ministry's representative in Grozny, as saying local bodyguards returned fire and managed to break through the blockade.\", '\"The car was hit by six bullets,\" Interfax quoted the general as saying. \"Luckily none of those in the car was hurt.\"', \"The incident on Grozny's central Minutka square came just hours after Russian and Chechen officials signed a deal detailing how they would repair a war-damaged oil pipeline that runs across Chechnya.\", \"The pipeline is a vital conduit for Azeri oil from Caspian Sea fields to reach world markets through Russia's Black Sea port of Novorossiisk.\", \"The general told Interfax the car was carrying local bodyguards and Vladimir Skobtsov, who works for a Russian commission set up to trace missing soldiers and prisoners of war. Russia's two-year conflict with Chechnya ended last August.\", \"Chechnya has established itself as a de facto independent entity since the war but Moscow still considers it to be one of Russia's 89 regions.\", 'Interfax said the general was trying to contact Chechen Interior Ministry officials to launch an investigation.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "n_train = 10\n",
    "n_test = 10\n",
    "\n",
    "data_list = os.listdir(data_path)\n",
    "n_samples = len(data_list)\n",
    "random_indices = random.sample(range(n_samples), n_train + n_test)\n",
    "\n",
    "train_indices = random_indices[0:n_train]\n",
    "test_indices = random_indices[n_train:(n_train + n_test)]\n",
    "\n",
    "train_list = [data_list[i] for i in train_indices]\n",
    "test_list = [data_list[i] for i in test_indices]\n",
    "\n",
    "news_train = []\n",
    "tags_train = []\n",
    "for file_name in train_list:\n",
    "    file_xml = data_path + file_name \n",
    "    (sentences, tags) = read_xml_file(file_xml)\n",
    "    \n",
    "    news_train.append(sentences)\n",
    "    tags_train.append(tags)\n",
    "\n",
    "print(tags_train[0:3], '\\n')\n",
    "for i in range(3):\n",
    "    print(news_train[i], '\\n')\n",
    "\n",
    "news_test = []\n",
    "tags_test = []\n",
    "for file_name in test_list:\n",
    "    file_xml = data_path + file_name \n",
    "    (sentences, tags) = read_xml_file(file_xml)\n",
    "    \n",
    "    news_test.append(sentences)\n",
    "    tags_test.append(tags)\n",
    "\n",
    "print(tags_test[0:3], '\\n')\n",
    "for i in range(3):\n",
    "    print(news_test[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will convert the training and test sets into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 791)\n",
      "(10, 791)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import itertools\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "words_train = [' '.join(news_item) for news_item in news_train] # concatenate each news item into a single string\n",
    "tokenizer.fit_on_texts(words_train)\n",
    "matrix_train = tokenizer.texts_to_matrix(words_train)\n",
    "\n",
    "words_test = [' '.join(news_item) for news_item in news_test] \n",
    "matrix_test = tokenizer.texts_to_matrix(words_test)\n",
    "\n",
    "print(matrix_train.shape)\n",
    "print(matrix_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['integrated', 'packaging', 'names', 'larrenaga', 'as', 'cfo'] \n",
      "\n",
      "['integrated', 'packaging', 'assembly', 'corp', 'said', 'on', 'tuesday', 'it', 'appointed', 'alfred', 'larrenaga', 'as', 'vice', 'president', 'and', 'chief', 'financial', 'officer'] \n",
      "\n",
      "['larrenaga', 'was', 'formerly', 'the', 'senior', 'vice', 'president', 'and', 'chief', 'financial', 'officer', 'of', 'southwall', 'technologies', 'inc', 'and', 'prior', 'to', 'that', 'vice', 'president', 'and', 'chief', 'financial', 'officer', 'of', 'asyst', 'technologies', 'inc'] \n",
      "\n",
      "['larrenaga', 'replaces', 'tony', 'lin', 'the', \"company's\", 'cfo', 'since', 'its', 'inception', 'in', '1993'] \n",
      "\n",
      "['integrated', 'packaging', 'is', 'a', 'semiconductor', 'packaging', 'foundry', 'which', 'gets', 'wafers', 'from', 'customers', 'and', 'assembles', 'and', 'encases', 'each', 'integrated', 'circuit', 'in', 'a', 'plastic', 'package'] \n",
      "\n",
      "\n",
      "\n",
      " Counter({'and': 6, 'integrated': 4, 'packaging': 4, 'larrenaga': 4, 'vice': 3, 'president': 3, 'chief': 3, 'financial': 3, 'officer': 3, 'as': 2, 'cfo': 2, 'the': 2, 'of': 2, 'technologies': 2, 'inc': 2, 'in': 2, 'a': 2, 'names': 1, 'assembly': 1, 'corp': 1, 'said': 1, 'on': 1, 'tuesday': 1, 'it': 1, 'appointed': 1, 'alfred': 1, 'was': 1, 'formerly': 1, 'senior': 1, 'southwall': 1, 'prior': 1, 'to': 1, 'that': 1, 'asyst': 1, 'replaces': 1, 'tony': 1, 'lin': 1, \"company's\": 1, 'since': 1, 'its': 1, 'inception': 1, '1993': 1, 'is': 1, 'semiconductor': 1, 'foundry': 1, 'which': 1, 'gets': 1, 'wafers': 1, 'from': 1, 'customers': 1, 'assembles': 1, 'encases': 1, 'each': 1, 'circuit': 1, 'plastic': 1, 'package': 1})\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "words = []\n",
    "for s in sentences:\n",
    "    words.append(text_to_word_sequence(s))\n",
    "\n",
    "for w in words:\n",
    "    print(w, '\\n')\n",
    "    \n",
    "word_counts = Counter(itertools.chain(*words))\n",
    "print('\\n\\n', word_counts)\n",
    "\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "vocabulary_inv = list(sorted(vocabulary_inv))\n",
    "\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change also the target variable into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1POL' '2ECO' '3SPO' '4GEN' '6INS' '7RSK' '8YDB' '9BNX' 'ADS10' 'BNW14'\n",
      " 'BRP11' 'C11' 'C12' 'C13' 'C14' 'C15' 'C151' 'C1511' 'C152' 'C16' 'C17'\n",
      " 'C171' 'C172' 'C173' 'C174' 'C18' 'C181' 'C182' 'C183' 'C21' 'C22' 'C23'\n",
      " 'C24' 'C31' 'C311' 'C312' 'C313' 'C32' 'C33' 'C331' 'C34' 'C41' 'C411'\n",
      " 'C42' 'CCAT' 'E11' 'E12' 'E121' 'E13' 'E131' 'E132' 'E14' 'E141' 'E142'\n",
      " 'E143' 'E21' 'E211' 'E212' 'E31' 'E311' 'E312' 'E313' 'E41' 'E411' 'E51'\n",
      " 'E511' 'E512' 'E513' 'E61' 'E71' 'ECAT' 'ENT12' 'G11' 'G111' 'G112' 'G113'\n",
      " 'G12' 'G13' 'G131' 'G14' 'G15' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'GCAT' 'GCRIM' 'GDEF' 'GDIP' 'GDIS' 'GEDU' 'GENT'\n",
      " 'GENV' 'GFAS' 'GHEA' 'GJOB' 'GMIL' 'GOBIT' 'GODD' 'GPOL' 'GPRO' 'GREL'\n",
      " 'GSCI' 'GSPO' 'GTOUR' 'GVIO' 'GVOTE' 'GWEA' 'GWELF' 'M11' 'M12' 'M13'\n",
      " 'M131' 'M132' 'M14' 'M141' 'M142' 'M143' 'MCAT' 'MEUR' 'PRB13'] \n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0] \n",
      "\n",
      "['M14', 'M141', 'MCAT'] \n",
      "\n",
      "(10, 126)\n",
      "(10, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(topics)\n",
    "y_train = mlb.fit_transform(tags_train)\n",
    "y_test = mlb.fit_transform(tags_test)\n",
    "\n",
    "print(mlb.classes_, '\\n')\n",
    "print(y_train[0], '\\n')\n",
    "print(tags_train[0], '\\n')\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "Finally, save your best model to the competition and return it as an `h5` file. For example like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` (e.g., by calling `y=model.predict(x_test)`) you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

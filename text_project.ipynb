{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Wednesday December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory, but if you use the keras.embedding layer, it will be more efficient. \n",
    "- Loading all documents into one big matrix as we have done in the exercises is not feasible (e.g. the virtual servers in CSC have only 3 GB of RAM). You need to load the documents in smaller chunks for the training. This shouldn't be a problem, as we are doing mini-batch training anyway, and thus we don't need to keep all the documents in memory. You can simply pass you current chunk of documents to `model.fit()` as it remembers the weights from the previous run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/DJKesoil/Opiskelu/deep/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set already downloaded.\n",
      "\n",
      "Data set already unzipped.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.data_utils import get_file\n",
    "import os\n",
    "import zipfile\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "database_path = 'train/'\n",
    "corpus_path = database_path + 'REUTERS_CORPUS_2/'\n",
    "data_path = corpus_path + 'data/'\n",
    "codes_path = corpus_path + 'codes/'\n",
    "\n",
    "if not os.path.exists(database_path):\n",
    "    dl_file='reuters.zip'\n",
    "    dl_url='https://www.cs.helsinki.fi/u/jgpyykko/'\n",
    "    get_file(dl_file, dl_url+dl_file, cache_dir='./', cache_subdir=database_path, extract=True)\n",
    "else:\n",
    "    print('Data set already downloaded.')\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    print('\\n\\nUnzipping data...')\n",
    "    \n",
    "    codes_zip = corpus_path + 'codes.zip'\n",
    "    with zipfile.ZipFile(codes_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(codes_path)\n",
    "    os.remove(codes_zip)\n",
    "   \n",
    "    dtds_zip = corpus_path + 'dtds.zip'\n",
    "    with zipfile.ZipFile(dtds_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(corpus_path + 'dtds/')\n",
    "    os.remove(dtds_zip)\n",
    "    \n",
    "    for item in os.listdir(corpus_path): \n",
    "        if item.endswith('zip'):\n",
    "            file_name = corpus_path + item \n",
    "            with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "                zip_ref.extractall(data_path)\n",
    "            os.remove(file_name) \n",
    "    \n",
    "    print('Data set unzipped.')\n",
    "else:\n",
    "    print('\\nData set already unzipped.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "First we will read the codes into the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126  different classes\n",
      "\n",
      "\n",
      "['1POL', '2ECO', '3SPO', '4GEN', '6INS', '7RSK', '8YDB', '9BNX', 'ADS10', 'BNW14', 'BRP11', 'C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16', 'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182', 'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313', 'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11', 'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142', 'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313', 'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT', 'ENT12', 'G11', 'G111', 'G112', 'G113', 'G12', 'G13', 'G131', 'G14', 'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157', 'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GEDU', 'GENT', 'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL', 'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA', 'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141', 'M142', 'M143', 'MCAT', 'MEUR', 'PRB13'] \n",
      "\n",
      "\n",
      "{'1POL': 0, '2ECO': 1, '3SPO': 2, '4GEN': 3, '6INS': 4, '7RSK': 5, '8YDB': 6, '9BNX': 7, 'ADS10': 8, 'BNW14': 9, 'BRP11': 10, 'C11': 11, 'C12': 12, 'C13': 13, 'C14': 14, 'C15': 15, 'C151': 16, 'C1511': 17, 'C152': 18, 'C16': 19, 'C17': 20, 'C171': 21, 'C172': 22, 'C173': 23, 'C174': 24, 'C18': 25, 'C181': 26, 'C182': 27, 'C183': 28, 'C21': 29, 'C22': 30, 'C23': 31, 'C24': 32, 'C31': 33, 'C311': 34, 'C312': 35, 'C313': 36, 'C32': 37, 'C33': 38, 'C331': 39, 'C34': 40, 'C41': 41, 'C411': 42, 'C42': 43, 'CCAT': 44, 'E11': 45, 'E12': 46, 'E121': 47, 'E13': 48, 'E131': 49, 'E132': 50, 'E14': 51, 'E141': 52, 'E142': 53, 'E143': 54, 'E21': 55, 'E211': 56, 'E212': 57, 'E31': 58, 'E311': 59, 'E312': 60, 'E313': 61, 'E41': 62, 'E411': 63, 'E51': 64, 'E511': 65, 'E512': 66, 'E513': 67, 'E61': 68, 'E71': 69, 'ECAT': 70, 'ENT12': 71, 'G11': 72, 'G111': 73, 'G112': 74, 'G113': 75, 'G12': 76, 'G13': 77, 'G131': 78, 'G14': 79, 'G15': 80, 'G151': 81, 'G152': 82, 'G153': 83, 'G154': 84, 'G155': 85, 'G156': 86, 'G157': 87, 'G158': 88, 'G159': 89, 'GCAT': 90, 'GCRIM': 91, 'GDEF': 92, 'GDIP': 93, 'GDIS': 94, 'GEDU': 95, 'GENT': 96, 'GENV': 97, 'GFAS': 98, 'GHEA': 99, 'GJOB': 100, 'GMIL': 101, 'GOBIT': 102, 'GODD': 103, 'GPOL': 104, 'GPRO': 105, 'GREL': 106, 'GSCI': 107, 'GSPO': 108, 'GTOUR': 109, 'GVIO': 110, 'GVOTE': 111, 'GWEA': 112, 'GWELF': 113, 'M11': 114, 'M12': 115, 'M13': 116, 'M131': 117, 'M132': 118, 'M14': 119, 'M141': 120, 'M142': 121, 'M143': 122, 'MCAT': 123, 'MEUR': 124, 'PRB13': 125} \n",
      "\n",
      "\n",
      "1POL  :  CURRENT NEWS - POLITICS\n",
      "2ECO  :  CURRENT NEWS - ECONOMICS\n",
      "3SPO  :  CURRENT NEWS - SPORT\n",
      "4GEN  :  CURRENT NEWS - GENERAL\n",
      "6INS  :  CURRENT NEWS - INSURANCE\n",
      "7RSK  :  CURRENT NEWS - RISK NEWS\n",
      "8YDB  :  TEMPORARY\n",
      "9BNX  :  TEMPORARY\n",
      "ADS10  :  CURRENT NEWS - ADVERTISING\n",
      "BNW14  :  CURRENT NEWS - BUSINESS NEWS\n",
      "BRP11  :  CURRENT NEWS - BRANDS\n",
      "C11  :  STRATEGY/PLANS\n",
      "C12  :  LEGAL/JUDICIAL\n",
      "C13  :  REGULATION/POLICY\n",
      "C14  :  SHARE LISTINGS\n",
      "C15  :  PERFORMANCE\n",
      "C151  :  ACCOUNTS/EARNINGS\n",
      "C1511  :  ANNUAL RESULTS\n",
      "C152  :  COMMENT/FORECASTS\n",
      "C16  :  INSOLVENCY/LIQUIDITY\n",
      "C17  :  FUNDING/CAPITAL\n",
      "C171  :  SHARE CAPITAL\n",
      "C172  :  BONDS/DEBT ISSUES\n",
      "C173  :  LOANS/CREDITS\n",
      "C174  :  CREDIT RATINGS\n",
      "C18  :  OWNERSHIP CHANGES\n",
      "C181  :  MERGERS/ACQUISITIONS\n",
      "C182  :  ASSET TRANSFERS\n",
      "C183  :  PRIVATISATIONS\n",
      "C21  :  PRODUCTION/SERVICES\n",
      "C22  :  NEW PRODUCTS/SERVICES\n",
      "C23  :  RESEARCH/DEVELOPMENT\n",
      "C24  :  CAPACITY/FACILITIES\n",
      "C31  :  MARKETS/MARKETING\n",
      "C311  :  DOMESTIC MARKETS\n",
      "C312  :  EXTERNAL MARKETS\n",
      "C313  :  MARKET SHARE\n",
      "C32  :  ADVERTISING/PROMOTION\n",
      "C33  :  CONTRACTS/ORDERS\n",
      "C331  :  DEFENCE CONTRACTS\n",
      "C34  :  MONOPOLIES/COMPETITION\n",
      "C41  :  MANAGEMENT\n",
      "C411  :  MANAGEMENT MOVES\n",
      "C42  :  LABOUR\n",
      "CCAT  :  CORPORATE/INDUSTRIAL\n",
      "E11  :  ECONOMIC PERFORMANCE\n",
      "E12  :  MONETARY/ECONOMIC\n",
      "E121  :  MONEY SUPPLY\n",
      "E13  :  INFLATION/PRICES\n",
      "E131  :  CONSUMER PRICES\n",
      "E132  :  WHOLESALE PRICES\n",
      "E14  :  CONSUMER FINANCE\n",
      "E141  :  PERSONAL INCOME\n",
      "E142  :  CONSUMER CREDIT\n",
      "E143  :  RETAIL SALES\n",
      "E21  :  GOVERNMENT FINANCE\n",
      "E211  :  EXPENDITURE/REVENUE\n",
      "E212  :  GOVERNMENT BORROWING\n",
      "E31  :  OUTPUT/CAPACITY\n",
      "E311  :  INDUSTRIAL PRODUCTION\n",
      "E312  :  CAPACITY UTILIZATION\n",
      "E313  :  INVENTORIES\n",
      "E41  :  EMPLOYMENT/LABOUR\n",
      "E411  :  UNEMPLOYMENT\n",
      "E51  :  TRADE/RESERVES\n",
      "E511  :  BALANCE OF PAYMENTS\n",
      "E512  :  MERCHANDISE TRADE\n",
      "E513  :  RESERVES\n",
      "E61  :  HOUSING STARTS\n",
      "E71  :  LEADING INDICATORS\n",
      "ECAT  :  ECONOMICS\n",
      "ENT12  :  CURRENT NEWS - ENTERTAINMENT\n",
      "G11  :  SOCIAL AFFAIRS\n",
      "G111  :  HEALTH/SAFETY\n",
      "G112  :  SOCIAL SECURITY\n",
      "G113  :  EDUCATION/RESEARCH\n",
      "G12  :  INTERNAL POLITICS\n",
      "G13  :  INTERNATIONAL RELATIONS\n",
      "G131  :  DEFENCE\n",
      "G14  :  ENVIRONMENT\n",
      "G15  :  EUROPEAN COMMUNITY\n",
      "G151  :  EC INTERNAL MARKET\n",
      "G152  :  EC CORPORATE POLICY\n",
      "G153  :  EC AGRICULTURE POLICY\n",
      "G154  :  EC MONETARY/ECONOMIC\n",
      "G155  :  EC INSTITUTIONS\n",
      "G156  :  EC ENVIRONMENT ISSUES\n",
      "G157  :  EC COMPETITION/SUBSIDY\n",
      "G158  :  EC EXTERNAL RELATIONS\n",
      "G159  :  EC GENERAL\n",
      "GCAT  :  GOVERNMENT/SOCIAL\n",
      "GCRIM  :  CRIME, LAW ENFORCEMENT\n",
      "GDEF  :  DEFENCE\n",
      "GDIP  :  INTERNATIONAL RELATIONS\n",
      "GDIS  :  DISASTERS AND ACCIDENTS\n",
      "GEDU  :  EDUCATION\n",
      "GENT  :  ARTS, CULTURE, ENTERTAINMENT\n",
      "GENV  :  ENVIRONMENT AND NATURAL WORLD\n",
      "GFAS  :  FASHION\n",
      "GHEA  :  HEALTH\n",
      "GJOB  :  LABOUR ISSUES\n",
      "GMIL  :  MILLENNIUM ISSUES\n",
      "GOBIT  :  OBITUARIES\n",
      "GODD  :  HUMAN INTEREST\n",
      "GPOL  :  DOMESTIC POLITICS\n",
      "GPRO  :  BIOGRAPHIES, PERSONALITIES, PEOPLE\n",
      "GREL  :  RELIGION\n",
      "GSCI  :  SCIENCE AND TECHNOLOGY\n",
      "GSPO  :  SPORTS\n",
      "GTOUR  :  TRAVEL AND TOURISM\n",
      "GVIO  :  WAR, CIVIL WAR\n",
      "GVOTE  :  ELECTIONS\n",
      "GWEA  :  WEATHER\n",
      "GWELF  :  WELFARE, SOCIAL SERVICES\n",
      "M11  :  EQUITY MARKETS\n",
      "M12  :  BOND MARKETS\n",
      "M13  :  MONEY MARKETS\n",
      "M131  :  INTERBANK MARKETS\n",
      "M132  :  FOREX MARKETS\n",
      "M14  :  COMMODITY MARKETS\n",
      "M141  :  SOFT COMMODITIES\n",
      "M142  :  METALS TRADING\n",
      "M143  :  ENERGY MARKETS\n",
      "MCAT  :  MARKETS\n",
      "MEUR  :  EURO CURRENCY\n",
      "PRB13  :  CURRENT NEWS - PRESS RELEASE WIRES\n",
      "1POL  :  0\n",
      "2ECO  :  1\n",
      "3SPO  :  2\n",
      "4GEN  :  3\n",
      "6INS  :  4\n",
      "7RSK  :  5\n",
      "8YDB  :  6\n",
      "9BNX  :  7\n",
      "ADS10  :  8\n",
      "BNW14  :  9\n",
      "BRP11  :  10\n",
      "C11  :  11\n",
      "C12  :  12\n",
      "C13  :  13\n",
      "C14  :  14\n",
      "C15  :  15\n",
      "C151  :  16\n",
      "C1511  :  17\n",
      "C152  :  18\n",
      "C16  :  19\n",
      "C17  :  20\n",
      "C171  :  21\n",
      "C172  :  22\n",
      "C173  :  23\n",
      "C174  :  24\n",
      "C18  :  25\n",
      "C181  :  26\n",
      "C182  :  27\n",
      "C183  :  28\n",
      "C21  :  29\n",
      "C22  :  30\n",
      "C23  :  31\n",
      "C24  :  32\n",
      "C31  :  33\n",
      "C311  :  34\n",
      "C312  :  35\n",
      "C313  :  36\n",
      "C32  :  37\n",
      "C33  :  38\n",
      "C331  :  39\n",
      "C34  :  40\n",
      "C41  :  41\n",
      "C411  :  42\n",
      "C42  :  43\n",
      "CCAT  :  44\n",
      "E11  :  45\n",
      "E12  :  46\n",
      "E121  :  47\n",
      "E13  :  48\n",
      "E131  :  49\n",
      "E132  :  50\n",
      "E14  :  51\n",
      "E141  :  52\n",
      "E142  :  53\n",
      "E143  :  54\n",
      "E21  :  55\n",
      "E211  :  56\n",
      "E212  :  57\n",
      "E31  :  58\n",
      "E311  :  59\n",
      "E312  :  60\n",
      "E313  :  61\n",
      "E41  :  62\n",
      "E411  :  63\n",
      "E51  :  64\n",
      "E511  :  65\n",
      "E512  :  66\n",
      "E513  :  67\n",
      "E61  :  68\n",
      "E71  :  69\n",
      "ECAT  :  70\n",
      "ENT12  :  71\n",
      "G11  :  72\n",
      "G111  :  73\n",
      "G112  :  74\n",
      "G113  :  75\n",
      "G12  :  76\n",
      "G13  :  77\n",
      "G131  :  78\n",
      "G14  :  79\n",
      "G15  :  80\n",
      "G151  :  81\n",
      "G152  :  82\n",
      "G153  :  83\n",
      "G154  :  84\n",
      "G155  :  85\n",
      "G156  :  86\n",
      "G157  :  87\n",
      "G158  :  88\n",
      "G159  :  89\n",
      "GCAT  :  90\n",
      "GCRIM  :  91\n",
      "GDEF  :  92\n",
      "GDIP  :  93\n",
      "GDIS  :  94\n",
      "GEDU  :  95\n",
      "GENT  :  96\n",
      "GENV  :  97\n",
      "GFAS  :  98\n",
      "GHEA  :  99\n",
      "GJOB  :  100\n",
      "GMIL  :  101\n",
      "GOBIT  :  102\n",
      "GODD  :  103\n",
      "GPOL  :  104\n",
      "GPRO  :  105\n",
      "GREL  :  106\n",
      "GSCI  :  107\n",
      "GSPO  :  108\n",
      "GTOUR  :  109\n",
      "GVIO  :  110\n",
      "GVOTE  :  111\n",
      "GWEA  :  112\n",
      "GWELF  :  113\n",
      "M11  :  114\n",
      "M12  :  115\n",
      "M13  :  116\n",
      "M131  :  117\n",
      "M132  :  118\n",
      "M14  :  119\n",
      "M141  :  120\n",
      "M142  :  121\n",
      "M143  :  122\n",
      "MCAT  :  123\n",
      "MEUR  :  124\n",
      "PRB13  :  125\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "topic_labels = {}\n",
    "codes_file = codes_path + 'topic_codes.txt'\n",
    "with open(codes_file) as f:\n",
    "    for line in f:\n",
    "        if not line.startswith(';'):\n",
    "            splits = line.split()\n",
    "            topic_code = splits[0]\n",
    "            topic_labels[topic_code] = ' '.join(splits[1:len(splits)])\n",
    "            topics.append(topic_code)\n",
    "\n",
    "n_class = len(topics)\n",
    "topic_index = {topics[i] : i for i in range(n_class)}\n",
    "\n",
    "\n",
    "print(n_class, ' different classes\\n\\n')\n",
    "print(topics, '\\n\\n')\n",
    "print(topic_index, '\\n\\n')\n",
    "\n",
    "for key in topic_labels:\n",
    "    print(key, ' : ', topic_labels[key])\n",
    "\n",
    "for key in topic_index:\n",
    "    print(key, ' : ', topic_index[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will parse the xml files. Let's first try to parse one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "tags:  ['GCAT', 'GVIO'] \n",
      "\n",
      "\n",
      "sentences:  ['Sri Lanka rebels attack government troops in north.', \"Heavy fighting erupted between government troops and Liberation Tigers of Tamil Eelam (LTTE) rebels in Sri Lanka's northern Wanni region late on Friday, military officials said on Saturday.\", \"They said the rebels had attacked the military's defensive positions just north of the government-held town of Vavuniya, some 220 km (135 miles) north of the capital Colombo.\", 'The military, which launched a major offensive in May, is battling rebels in the Wanni to open a strategic highway linking the northern Jaffna peninsula with the rest of the island.', \"The military officials said defences guarded by police and the navy had been breached but troops had linked up again after repulsing Friday's attack.\", 'Casualty figures and other details were not immediately available, but officials said troops were clearing the area.', 'An undisclosed number of wounded had been airlifted to Anuradhapura military base, some 50 km (31 miles) south of Vavuniya, officials said.', 'A Defence Ministry spokesman said in Colombo confirmed the attacks and said the situation was now under control.', 'Residents in Vavuniya said troops were shelling the affected area on Saturday morning.', '\"Shelling is still continuing and helicopters and air force planes are flying by,\" a resident said by phone from Vavuniya.', 'He said the fighting started at about 10.30 p.m. (1630 GMT Friday) and continued through most of the night.', \"The military's northward offensive has slowed since the LTTE launched two fierce counterattacks in June but the fighting has shown signs of escalating this week.\", \"Sri Lanka's Defence Ministry said on Friday that troops had clashed with a large number of LTTE rebels southwest of Nedunkeni town on Thursday.\", 'At least 50 guerrillas and 17 troops, including one officer, was killed in the gunbattle. Troops had called in helicopter gunships to engage the rebels, who fled northwards with their wounded, the statement said.', '\"Sri Lanka forces backed by heavy artillery and moving in battle-tanks and armoured vehicles had attempted a sudden push towards their goal of Puliyankulam, but LTTE forces hit back decisively,\" an LTTE statement said.', \"The LTTE have been fighting for a separate homeland for minority Tamils in Sri Lanka's north and east since 1983.\", 'The government says more than 50,000 people have been killed in the war. The LTTE say the toll is higher.'] \n",
      "\n",
      "\n",
      "Sri Lanka rebels attack government troops in north. \n",
      "\n",
      "Heavy fighting erupted between government troops and Liberation Tigers of Tamil Eelam (LTTE) rebels in Sri Lanka's northern Wanni region late on Friday, military officials said on Saturday. \n",
      "\n",
      "They said the rebels had attacked the military's defensive positions just north of the government-held town of Vavuniya, some 220 km (135 miles) north of the capital Colombo. \n",
      "\n",
      "The military, which launched a major offensive in May, is battling rebels in the Wanni to open a strategic highway linking the northern Jaffna peninsula with the rest of the island. \n",
      "\n",
      "The military officials said defences guarded by police and the navy had been breached but troops had linked up again after repulsing Friday's attack. \n",
      "\n",
      "Casualty figures and other details were not immediately available, but officials said troops were clearing the area. \n",
      "\n",
      "An undisclosed number of wounded had been airlifted to Anuradhapura military base, some 50 km (31 miles) south of Vavuniya, officials said. \n",
      "\n",
      "A Defence Ministry spokesman said in Colombo confirmed the attacks and said the situation was now under control. \n",
      "\n",
      "Residents in Vavuniya said troops were shelling the affected area on Saturday morning. \n",
      "\n",
      "\"Shelling is still continuing and helicopters and air force planes are flying by,\" a resident said by phone from Vavuniya. \n",
      "\n",
      "He said the fighting started at about 10.30 p.m. (1630 GMT Friday) and continued through most of the night. \n",
      "\n",
      "The military's northward offensive has slowed since the LTTE launched two fierce counterattacks in June but the fighting has shown signs of escalating this week. \n",
      "\n",
      "Sri Lanka's Defence Ministry said on Friday that troops had clashed with a large number of LTTE rebels southwest of Nedunkeni town on Thursday. \n",
      "\n",
      "At least 50 guerrillas and 17 troops, including one officer, was killed in the gunbattle. Troops had called in helicopter gunships to engage the rebels, who fled northwards with their wounded, the statement said. \n",
      "\n",
      "\"Sri Lanka forces backed by heavy artillery and moving in battle-tanks and armoured vehicles had attempted a sudden push towards their goal of Puliyankulam, but LTTE forces hit back decisively,\" an LTTE statement said. \n",
      "\n",
      "The LTTE have been fighting for a separate homeland for minority Tamils in Sri Lanka's north and east since 1983. \n",
      "\n",
      "The government says more than 50,000 people have been killed in the war. The LTTE say the toll is higher. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "filename_xml = '810900newsML.xml'\n",
    "file_xml = data_path + filename_xml\n",
    "\n",
    "def read_xml_file(file_xml):\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    read_tags = False\n",
    "    for event, elem in etree.iterparse(file_xml, events=('start', 'end')):\n",
    "        t = elem.tag\n",
    "        idx = k = t.rfind(\"}\")\n",
    "        if idx != -1:\n",
    "            t = t[idx + 1:]\n",
    "        tname = t\n",
    "\n",
    "        if event == 'start':\n",
    "            if tname == 'codes':\n",
    "                if elem.attrib['class'] == 'bip:topics:1.0':\n",
    "                    read_tags = True\n",
    "            if tname == 'code':\n",
    "                if read_tags:\n",
    "                    tags.append(elem.attrib['code'])\n",
    "    \n",
    "        if event == 'end':\n",
    "            if tname == 'headline':\n",
    "                sentences.append(elem.text)\n",
    "            if tname == 'p':\n",
    "                sentences.append(elem.text)\n",
    "            if tname == 'codes':\n",
    "                if elem.attrib['class'] == 'bip:topics:1.0':\n",
    "                    read_tags = False\n",
    "\n",
    "    return [sentences, tags]\n",
    "    \n",
    "(sentences, tags) = read_xml_file(file_xml)\n",
    "\n",
    "print('\\n\\ntags: ', tags, '\\n\\n')\n",
    "print('sentences: ', sentences, '\\n\\n')\n",
    "for i in range(len(sentences)):\n",
    "    print(sentences[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read a small training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags is none:  706251newsML.xml\n",
      "tags is none:  697400newsML.xml\n",
      "tags is none:  585031newsML.xml\n",
      "tags is none:  651293newsML.xml\n",
      "tags is none:  609850newsML.xml\n",
      "tags is none:  606259newsML.xml\n",
      "tags is none:  563666newsML.xml\n",
      "tags is none:  668844newsML.xml\n",
      "tags is none:  482577newsML.xml\n",
      "tags is none:  777648newsML.xml\n",
      "tags is none:  761139newsML.xml\n",
      "tags is none:  708735newsML.xml\n",
      "tags is none:  600770newsML.xml\n",
      "tags is none:  730559newsML.xml\n",
      "tags is none:  798319newsML.xml\n",
      "tags is none:  786219newsML.xml\n",
      "tags is none:  573968newsML.xml\n",
      "tags is none:  612069newsML.xml\n",
      "tags is none:  690740newsML.xml\n",
      "tags is none:  740353newsML.xml\n",
      "tags is none:  690729newsML.xml\n",
      "tags is none:  723584newsML.xml\n",
      "tags is none:  797696newsML.xml\n",
      "tags is none:  658558newsML.xml\n",
      "tags is none:  486021newsML.xml\n",
      "tags is none:  612083newsML.xml\n",
      "tags is none:  561516newsML.xml\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "n_train = 10000\n",
    "n_test = 10000\n",
    "\n",
    "data_list = os.listdir(data_path)\n",
    "n_samples = len(data_list)\n",
    "random_indices = random.sample(range(n_samples), n_train + n_test)\n",
    "\n",
    "train_indices = random_indices[0:n_train]\n",
    "test_indices = random_indices[n_train:(n_train + n_test)]\n",
    "\n",
    "train_list = [data_list[i] for i in train_indices]\n",
    "test_list = [data_list[i] for i in test_indices]\n",
    "\n",
    "news_train = []\n",
    "tags_train = []\n",
    "for file_name in train_list:\n",
    "    file_xml = data_path + file_name \n",
    "    (sentences, tags) = read_xml_file(file_xml)\n",
    "    \n",
    "    if not sentences:\n",
    "        print('sentences is none: ', file_name)\n",
    "    if not tags: \n",
    "        print('tags is none: ', file_name)\n",
    "    news_train.append(sentences)\n",
    "    tags_train.append(tags)\n",
    "\n",
    "# print(tags_train[0:3], '\\n')\n",
    "# for i in range(3):\n",
    "#     print(news_train[i], '\\n')\n",
    "\n",
    "news_test = []\n",
    "tags_test = []\n",
    "for file_name in test_list:\n",
    "    file_xml = data_path + file_name \n",
    "    (sentences, tags) = read_xml_file(file_xml)\n",
    "    \n",
    "    news_test.append(sentences)\n",
    "    tags_test.append(tags)\n",
    "\n",
    "# print(tags_test[0:3], '\\n')\n",
    "# for i in range(3):\n",
    "#     print(news_test[i], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will convert the training and test sets into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30000)\n",
      "(10000, 30000)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import itertools\n",
    "\n",
    "max_vocabulary = 30000\n",
    "tokenizer = Tokenizer(max_vocabulary)\n",
    "\n",
    "# concatenate each news item into a single string\n",
    "words_train = [' '.join(filter(None, news_item)) for news_item in news_train] \n",
    "\n",
    "# words_train = []\n",
    "# for i in range(n_train):\n",
    "#     if None in news_train[i]:\n",
    "#        words_train.append(' '.join(filter(None, news_train[i])))\n",
    "#        print('None in file : ', train_list[i], '\\n')\n",
    "#        print(news_train[i], '\\n')\n",
    "#    else:\n",
    "#        words_train.append(' '.join(news_train[i]))\n",
    "\n",
    "tokenizer.fit_on_texts(words_train)\n",
    "matrix_train = tokenizer.texts_to_matrix(words_train)\n",
    "\n",
    "words_test = [' '.join(filter(None, news_item)) for news_item in news_test] \n",
    "\n",
    "\n",
    "# words_test = []\n",
    "# for news_item in news_train:\n",
    " #    words_test.append(' '.join(news_item))\n",
    "\n",
    "matrix_test = tokenizer.texts_to_matrix(words_test)\n",
    "\n",
    "print(matrix_train.shape)\n",
    "print(matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change also the target variable into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1POL' '2ECO' '3SPO' '4GEN' '6INS' '7RSK' '8YDB' '9BNX' 'ADS10' 'BNW14'\n",
      " 'BRP11' 'C11' 'C12' 'C13' 'C14' 'C15' 'C151' 'C1511' 'C152' 'C16' 'C17'\n",
      " 'C171' 'C172' 'C173' 'C174' 'C18' 'C181' 'C182' 'C183' 'C21' 'C22' 'C23'\n",
      " 'C24' 'C31' 'C311' 'C312' 'C313' 'C32' 'C33' 'C331' 'C34' 'C41' 'C411'\n",
      " 'C42' 'CCAT' 'E11' 'E12' 'E121' 'E13' 'E131' 'E132' 'E14' 'E141' 'E142'\n",
      " 'E143' 'E21' 'E211' 'E212' 'E31' 'E311' 'E312' 'E313' 'E41' 'E411' 'E51'\n",
      " 'E511' 'E512' 'E513' 'E61' 'E71' 'ECAT' 'ENT12' 'G11' 'G111' 'G112' 'G113'\n",
      " 'G12' 'G13' 'G131' 'G14' 'G15' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'GCAT' 'GCRIM' 'GDEF' 'GDIP' 'GDIS' 'GEDU' 'GENT'\n",
      " 'GENV' 'GFAS' 'GHEA' 'GJOB' 'GMIL' 'GOBIT' 'GODD' 'GPOL' 'GPRO' 'GREL'\n",
      " 'GSCI' 'GSPO' 'GTOUR' 'GVIO' 'GVOTE' 'GWEA' 'GWELF' 'M11' 'M12' 'M13'\n",
      " 'M131' 'M132' 'M14' 'M141' 'M142' 'M143' 'MCAT' 'MEUR' 'PRB13'] \n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
      "\n",
      "['C33', 'CCAT'] \n",
      "\n",
      "(10000, 126)\n",
      "(10000, 126)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(topics)\n",
    "y_train = mlb.fit_transform(tags_train)\n",
    "y_test = mlb.fit_transform(tags_test)\n",
    "\n",
    "print(mlb.classes_, '\\n')\n",
    "print(y_train[0], '\\n')\n",
    "print(tags_train[0], '\\n')\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the F1 score that is our error metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import adam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, finally we can define a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               15360512  \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 126)               64638     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 126)               0         \n",
      "=================================================================\n",
      "Total params: 15,425,150\n",
      "Trainable params: 15,425,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_vocabulary,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training for some iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 36s 4ms/step - loss: 0.0218 - acc: 0.9928\n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 34s 3ms/step - loss: 0.0160 - acc: 0.9948\n",
      "CPU times: user 3min 5s, sys: 26.9 s, total: 3min 32s\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(matrix_train, \n",
    "                    y_train, \n",
    "                    epochs=2, \n",
    "                    batch_size=128,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7801\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(matrix_test, batch_size=128)\n",
    "pred_test = np.array(pred_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(y_test, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for the first point of test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False]\n",
      "[  3.54303374e-05   6.78189317e-05   3.37837810e-05   2.77145718e-05\n",
      "   1.43047801e-05   3.39538965e-05   2.03092295e-05   5.56876075e-05\n",
      "   5.39867797e-05   6.81306628e-05   3.40184561e-05   6.31556963e-04\n",
      "   6.23460510e-04   8.45046830e-04   8.39795917e-04   1.42861507e-03\n",
      "   1.16561993e-03   4.93294094e-04   6.62739098e-04   2.91552074e-04\n",
      "   7.81201583e-04   2.94815720e-04   1.17425145e-04   4.20122815e-04\n",
      "   1.09572822e-04   3.16824211e-04   3.57790384e-04   2.78749445e-04\n",
      "   3.15152836e-04   1.67771126e-03   5.84582798e-04   8.41326138e-04\n",
      "   1.35968311e-03   1.60321128e-03   2.34199673e-04   2.42442402e-04\n",
      "   1.00359274e-03   3.81428254e-04   9.34822718e-04   7.96901586e-04\n",
      "   1.01374695e-04   1.52021390e-03   1.28607755e-03   1.90925828e-04\n",
      "   8.77461582e-03   8.02099297e-04   1.03844400e-03   2.84788257e-04\n",
      "   2.22488452e-04   1.90798630e-04   1.85740748e-04   4.54370951e-04\n",
      "   2.91809003e-04   3.90984969e-05   3.94737435e-04   1.43610628e-03\n",
      "   7.60738389e-04   5.48922748e-04   2.92860612e-04   5.30832971e-04\n",
      "   6.97994474e-05   6.98971635e-05   3.76090728e-04   7.14722089e-04\n",
      "   2.27532699e-04   1.50337728e-04   1.93853135e-04   3.17836471e-04\n",
      "   2.65939801e-04   1.04360908e-04   2.15605204e-03   4.77206777e-05\n",
      "   4.42844066e-05   6.36138502e-05   4.36602786e-05   3.50999908e-05\n",
      "   3.56643104e-05   5.34396277e-05   3.99695091e-05   1.84048531e-05\n",
      "   5.73176134e-04   3.69446090e-04   5.71180135e-04   3.44160857e-04\n",
      "   4.83385782e-04   4.63641918e-04   8.21761132e-05   3.40909697e-04\n",
      "   2.06640863e-04   4.84353041e-05   9.38539207e-01   3.21824593e-03\n",
      "   1.12615235e-03   9.28165508e-04   1.57475774e-03   3.94659510e-05\n",
      "   2.39381846e-03   1.76405662e-03   5.59342676e-04   1.76192331e-03\n",
      "   4.50738851e-04   3.97805961e-05   3.02888011e-03   4.26536985e-03\n",
      "   4.25605383e-03   6.29805587e-03   9.96139250e-04   2.26940610e-03\n",
      "   9.44604456e-01   2.92134151e-04   2.50254897e-03   1.64042739e-03\n",
      "   1.34510756e-03   4.98466776e-04   1.17506622e-03   6.94766815e-04\n",
      "   7.52140942e-04   5.16299857e-04   4.85797646e-04   3.45208123e-03\n",
      "   1.40518486e-03   2.63337482e-04   4.18180745e-04   5.54366224e-03\n",
      "   9.82227721e-05   5.39759167e-05]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0])\n",
    "print(pred_test[0])\n",
    "print(prob_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "Finally, save your best model to the competition and return it as an `h5` file. For example like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` (e.g., by calling `y=model.predict(x_test)`) you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Wednesday December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory, but if you use the keras.embedding layer, it will be more efficient. \n",
    "- Loading all documents into one big matrix as we have done in the exercises is not feasible (e.g. the virtual servers in CSC have only 3 GB of RAM). You need to load the documents in smaller chunks for the training. This shouldn't be a problem, as we are doing mini-batch training anyway, and thus we don't need to keep all the documents in memory. You can simply pass you current chunk of documents to `model.fit()` as it remembers the weights from the previous run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "Let's first set some paths & download the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set already downloaded.\n",
      "Data set already unzipped.\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import download_data \n",
    "\n",
    "database_path = 'train/'\n",
    "corpus_path = database_path + 'REUTERS_CORPUS_2/'\n",
    "data_path = corpus_path + 'data/'\n",
    "codes_path = corpus_path + 'codes/'\n",
    "\n",
    "download_data(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data set\n",
    "First we will read the codes into the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126  different classes\n",
      "\n",
      "\n",
      "['1POL', '2ECO', '3SPO', '4GEN', '6INS', '7RSK', '8YDB', '9BNX', 'ADS10', 'BNW14', 'BRP11', 'C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16', 'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182', 'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313', 'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11', 'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142', 'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313', 'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT', 'ENT12', 'G11', 'G111', 'G112', 'G113', 'G12', 'G13', 'G131', 'G14', 'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157', 'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GEDU', 'GENT', 'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL', 'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA', 'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141', 'M142', 'M143', 'MCAT', 'MEUR', 'PRB13'] \n",
      "\n",
      "\n",
      "{'1POL': 0, '2ECO': 1, '3SPO': 2, '4GEN': 3, '6INS': 4, '7RSK': 5, '8YDB': 6, '9BNX': 7, 'ADS10': 8, 'BNW14': 9, 'BRP11': 10, 'C11': 11, 'C12': 12, 'C13': 13, 'C14': 14, 'C15': 15, 'C151': 16, 'C1511': 17, 'C152': 18, 'C16': 19, 'C17': 20, 'C171': 21, 'C172': 22, 'C173': 23, 'C174': 24, 'C18': 25, 'C181': 26, 'C182': 27, 'C183': 28, 'C21': 29, 'C22': 30, 'C23': 31, 'C24': 32, 'C31': 33, 'C311': 34, 'C312': 35, 'C313': 36, 'C32': 37, 'C33': 38, 'C331': 39, 'C34': 40, 'C41': 41, 'C411': 42, 'C42': 43, 'CCAT': 44, 'E11': 45, 'E12': 46, 'E121': 47, 'E13': 48, 'E131': 49, 'E132': 50, 'E14': 51, 'E141': 52, 'E142': 53, 'E143': 54, 'E21': 55, 'E211': 56, 'E212': 57, 'E31': 58, 'E311': 59, 'E312': 60, 'E313': 61, 'E41': 62, 'E411': 63, 'E51': 64, 'E511': 65, 'E512': 66, 'E513': 67, 'E61': 68, 'E71': 69, 'ECAT': 70, 'ENT12': 71, 'G11': 72, 'G111': 73, 'G112': 74, 'G113': 75, 'G12': 76, 'G13': 77, 'G131': 78, 'G14': 79, 'G15': 80, 'G151': 81, 'G152': 82, 'G153': 83, 'G154': 84, 'G155': 85, 'G156': 86, 'G157': 87, 'G158': 88, 'G159': 89, 'GCAT': 90, 'GCRIM': 91, 'GDEF': 92, 'GDIP': 93, 'GDIS': 94, 'GEDU': 95, 'GENT': 96, 'GENV': 97, 'GFAS': 98, 'GHEA': 99, 'GJOB': 100, 'GMIL': 101, 'GOBIT': 102, 'GODD': 103, 'GPOL': 104, 'GPRO': 105, 'GREL': 106, 'GSCI': 107, 'GSPO': 108, 'GTOUR': 109, 'GVIO': 110, 'GVOTE': 111, 'GWEA': 112, 'GWELF': 113, 'M11': 114, 'M12': 115, 'M13': 116, 'M131': 117, 'M132': 118, 'M14': 119, 'M141': 120, 'M142': 121, 'M143': 122, 'MCAT': 123, 'MEUR': 124, 'PRB13': 125} \n",
      "\n",
      "\n",
      "1POL  :  CURRENT NEWS - POLITICS\n",
      "2ECO  :  CURRENT NEWS - ECONOMICS\n",
      "3SPO  :  CURRENT NEWS - SPORT\n",
      "4GEN  :  CURRENT NEWS - GENERAL\n",
      "6INS  :  CURRENT NEWS - INSURANCE\n",
      "7RSK  :  CURRENT NEWS - RISK NEWS\n",
      "8YDB  :  TEMPORARY\n",
      "9BNX  :  TEMPORARY\n",
      "ADS10  :  CURRENT NEWS - ADVERTISING\n",
      "BNW14  :  CURRENT NEWS - BUSINESS NEWS\n",
      "BRP11  :  CURRENT NEWS - BRANDS\n",
      "C11  :  STRATEGY/PLANS\n",
      "C12  :  LEGAL/JUDICIAL\n",
      "C13  :  REGULATION/POLICY\n",
      "C14  :  SHARE LISTINGS\n",
      "C15  :  PERFORMANCE\n",
      "C151  :  ACCOUNTS/EARNINGS\n",
      "C1511  :  ANNUAL RESULTS\n",
      "C152  :  COMMENT/FORECASTS\n",
      "C16  :  INSOLVENCY/LIQUIDITY\n",
      "C17  :  FUNDING/CAPITAL\n",
      "C171  :  SHARE CAPITAL\n",
      "C172  :  BONDS/DEBT ISSUES\n",
      "C173  :  LOANS/CREDITS\n",
      "C174  :  CREDIT RATINGS\n",
      "C18  :  OWNERSHIP CHANGES\n",
      "C181  :  MERGERS/ACQUISITIONS\n",
      "C182  :  ASSET TRANSFERS\n",
      "C183  :  PRIVATISATIONS\n",
      "C21  :  PRODUCTION/SERVICES\n",
      "C22  :  NEW PRODUCTS/SERVICES\n",
      "C23  :  RESEARCH/DEVELOPMENT\n",
      "C24  :  CAPACITY/FACILITIES\n",
      "C31  :  MARKETS/MARKETING\n",
      "C311  :  DOMESTIC MARKETS\n",
      "C312  :  EXTERNAL MARKETS\n",
      "C313  :  MARKET SHARE\n",
      "C32  :  ADVERTISING/PROMOTION\n",
      "C33  :  CONTRACTS/ORDERS\n",
      "C331  :  DEFENCE CONTRACTS\n",
      "C34  :  MONOPOLIES/COMPETITION\n",
      "C41  :  MANAGEMENT\n",
      "C411  :  MANAGEMENT MOVES\n",
      "C42  :  LABOUR\n",
      "CCAT  :  CORPORATE/INDUSTRIAL\n",
      "E11  :  ECONOMIC PERFORMANCE\n",
      "E12  :  MONETARY/ECONOMIC\n",
      "E121  :  MONEY SUPPLY\n",
      "E13  :  INFLATION/PRICES\n",
      "E131  :  CONSUMER PRICES\n",
      "E132  :  WHOLESALE PRICES\n",
      "E14  :  CONSUMER FINANCE\n",
      "E141  :  PERSONAL INCOME\n",
      "E142  :  CONSUMER CREDIT\n",
      "E143  :  RETAIL SALES\n",
      "E21  :  GOVERNMENT FINANCE\n",
      "E211  :  EXPENDITURE/REVENUE\n",
      "E212  :  GOVERNMENT BORROWING\n",
      "E31  :  OUTPUT/CAPACITY\n",
      "E311  :  INDUSTRIAL PRODUCTION\n",
      "E312  :  CAPACITY UTILIZATION\n",
      "E313  :  INVENTORIES\n",
      "E41  :  EMPLOYMENT/LABOUR\n",
      "E411  :  UNEMPLOYMENT\n",
      "E51  :  TRADE/RESERVES\n",
      "E511  :  BALANCE OF PAYMENTS\n",
      "E512  :  MERCHANDISE TRADE\n",
      "E513  :  RESERVES\n",
      "E61  :  HOUSING STARTS\n",
      "E71  :  LEADING INDICATORS\n",
      "ECAT  :  ECONOMICS\n",
      "ENT12  :  CURRENT NEWS - ENTERTAINMENT\n",
      "G11  :  SOCIAL AFFAIRS\n",
      "G111  :  HEALTH/SAFETY\n",
      "G112  :  SOCIAL SECURITY\n",
      "G113  :  EDUCATION/RESEARCH\n",
      "G12  :  INTERNAL POLITICS\n",
      "G13  :  INTERNATIONAL RELATIONS\n",
      "G131  :  DEFENCE\n",
      "G14  :  ENVIRONMENT\n",
      "G15  :  EUROPEAN COMMUNITY\n",
      "G151  :  EC INTERNAL MARKET\n",
      "G152  :  EC CORPORATE POLICY\n",
      "G153  :  EC AGRICULTURE POLICY\n",
      "G154  :  EC MONETARY/ECONOMIC\n",
      "G155  :  EC INSTITUTIONS\n",
      "G156  :  EC ENVIRONMENT ISSUES\n",
      "G157  :  EC COMPETITION/SUBSIDY\n",
      "G158  :  EC EXTERNAL RELATIONS\n",
      "G159  :  EC GENERAL\n",
      "GCAT  :  GOVERNMENT/SOCIAL\n",
      "GCRIM  :  CRIME, LAW ENFORCEMENT\n",
      "GDEF  :  DEFENCE\n",
      "GDIP  :  INTERNATIONAL RELATIONS\n",
      "GDIS  :  DISASTERS AND ACCIDENTS\n",
      "GEDU  :  EDUCATION\n",
      "GENT  :  ARTS, CULTURE, ENTERTAINMENT\n",
      "GENV  :  ENVIRONMENT AND NATURAL WORLD\n",
      "GFAS  :  FASHION\n",
      "GHEA  :  HEALTH\n",
      "GJOB  :  LABOUR ISSUES\n",
      "GMIL  :  MILLENNIUM ISSUES\n",
      "GOBIT  :  OBITUARIES\n",
      "GODD  :  HUMAN INTEREST\n",
      "GPOL  :  DOMESTIC POLITICS\n",
      "GPRO  :  BIOGRAPHIES, PERSONALITIES, PEOPLE\n",
      "GREL  :  RELIGION\n",
      "GSCI  :  SCIENCE AND TECHNOLOGY\n",
      "GSPO  :  SPORTS\n",
      "GTOUR  :  TRAVEL AND TOURISM\n",
      "GVIO  :  WAR, CIVIL WAR\n",
      "GVOTE  :  ELECTIONS\n",
      "GWEA  :  WEATHER\n",
      "GWELF  :  WELFARE, SOCIAL SERVICES\n",
      "M11  :  EQUITY MARKETS\n",
      "M12  :  BOND MARKETS\n",
      "M13  :  MONEY MARKETS\n",
      "M131  :  INTERBANK MARKETS\n",
      "M132  :  FOREX MARKETS\n",
      "M14  :  COMMODITY MARKETS\n",
      "M141  :  SOFT COMMODITIES\n",
      "M142  :  METALS TRADING\n",
      "M143  :  ENERGY MARKETS\n",
      "MCAT  :  MARKETS\n",
      "MEUR  :  EURO CURRENCY\n",
      "PRB13  :  CURRENT NEWS - PRESS RELEASE WIRES\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import read_topics\n",
    "\n",
    "(topics, topic_index, topic_labels) = read_topics(database_path)\n",
    "n_class = len(topics)\n",
    "\n",
    "print(n_class, ' different classes\\n\\n')\n",
    "print(topics, '\\n\\n')           # topics codes as an array\n",
    "print(topic_index, '\\n\\n')      # dictionary of topic code : index of this code in \"topics array\"\n",
    "\n",
    "for key in topic_labels:        # dictionary of topic code : label of this topic code\n",
    "    print(key, ' : ', topic_labels[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read a small training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C15', 'C151', 'CCAT'], ['GCAT', 'GSPO'], ['E12', 'ECAT', 'M13', 'M132', 'MCAT']] \n",
      "\n",
      "['ICE HOCKEY-WORLD CHAMPIONSHIP STANDINGS.', 'Ice hockey world', \"championship standings after Monday's games:\", '    Pool A\\t\\t P   W   D   L   F   A  Pts', ' 1. Czech Republic     2   2   0   0   4   2   4', ' 2. Russia\\t\\t 2   1   1   0   7   3   3', ' 4. Slovakia\\t     2   1   1   0   7   5   3', ' 3. Finland\\t\\t2   1   0   1   7   3   2', ' 5. France\\t\\t 2   0   0   2   4  11   0', ' 6. Germany\\t\\t2   0   0   2   2   7   0', 'Pool B', ' 1. Sweden\\t\\t 2   2   0   0  12   5   4', ' 2. U.S.\\t\\t   2   2   0   0   8   5   4', ' 3. Canada\\t\\t 2   1   0   1   9   7   2', ' 4. Italy\\t\\t  2   1   0   1   8   9   2', ' 5. Latvia\\t\\t 2   0   0   2   8  10   0', ' 6. Norway\\t\\t 2   0   0   2   1  10   0', 'Note: Top three teams qualify for medal round'] \n",
      "\n",
      "[['M13', 'M131', 'MCAT'], ['M13', 'M131', 'MCAT'], ['C15', 'C151', 'CCAT']] \n",
      "\n",
      "['Canadian T-bills open mostly weaker in quiet trade.', 'Canadian T-bills opened mostly weaker in quiet trade on Tuesday, taking much of their tone from U.S. Treasuries as dealers returned to the market following a long weekend in most of Canada.', '\"I\\'ve seen a couple of sellers in the front end, nothing too huge though,\" said one T-bill dealer with a bank-owned brokerage. \"I think that the Canadian money market guys are going to start to play this thing more cautiously.\"', \"Canada's three-month cash T-bill softened to yield 3.28 percent from 3.27 percent from the close of trading on Friday.  \", \"Dealers said the money market took its direction from U.S. Treasuries, which weakened on Monday while fixed-income markets were closed in Ontario. Most money market trading takes place in Canada's most populous province.\", 'Looking ahead, one dealer said the medium- to long-term view remains bearish for Canadian T-bills, with the possibility of a second Bank of Canada rate hike keeping prices from advancing very far.', 'In other prices, the six-month cash T-bill softened to yield 3.62 percent from 3.61 percent on Friday. The one-year when-issued T-bill traded unchanged to yield 4.17 percent.  ', \"The call loan or overnight lending rate traded at 3.35 percent, in the upper half of the Bank of Canada's 3.0-3.5 percent target range.\", '((Jeffrey Hodgson (416) 941-8105, e-mail: jeffrey.hodgson@reuters.com))'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import read_news\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 10000\n",
    "\n",
    "(news_train_t, tags_train_t, news_test_t, tags_test_t) = read_news(database_path, n_train, n_test, seed = 1234)\n",
    "\n",
    "print(tags_train_t[0:3], '\\n')\n",
    "print(news_train_t[1], '\\n')\n",
    "\n",
    "print(tags_test_t[0:3], '\\n')\n",
    "print(news_test_t[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Zip found\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import download_glove\n",
    "embeddings_path = \"embeddings/\"\n",
    "download_glove(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already unzipped\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import unzip_glove\n",
    "zip_file_name = \"glove.6B.zip\"\n",
    "unzip_glove(embeddings_path, zip_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import get_glove_embeddings\n",
    "p=200\n",
    "embeddings = get_glove_embeddings(p, embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings[\"the\"]))\n",
    "print(len(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import process_data\n",
    "if not os.path.exists(\"train/REUTERS_CORPUS_2/tokenized/\"): process_data(\"train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import build_dictionary\n",
    "if not os.path.exists(\"dictionary.json\"): build_dictionary(\"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "word_to_index = json.loads(open(\"dictionary.json\").read())\n",
    "dict_size = len(word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import vectorize_data\n",
    "if not os.path.exists(\"train/REUTERS_CORPUS_2/vectorized/\"): vectorize_data(\"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import get_vectorized_data\n",
    "vectorized_data_path = \"train/REUTERS_CORPUS_2/vectorized/\"\n",
    "tags_path=\"train/REUTERS_CORPUS_2/tags/\"\n",
    "n_train=10000\n",
    "n_test=10000\n",
    "(news_train, tags_train, news_test, tags_test) = get_vectorized_data(vectorized_data_path, tags_path, n_train, n_test, seed = 1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9263 100301   7404   7511   9263   9294    460   7404   7511   3560\n",
      "   4090   9041   1242   8212   4484   5633   1344   2939   1242   4595\n",
      "      8   7597   7598      8      8      8      8      8      8      8\n",
      "      8   7855      8      8      8      8      8      8      8      8\n",
      "   7603      8      8      8      8      8      8      8      8   6701\n",
      "      8      8      8      8      8      8      8      8   1167      8\n",
      "      8      8      8      8      8      8      8   1168      8      8\n",
      "      8      8      8      8      8   9041    764      8   7582      8\n",
      "      8      8      8      8      8      8      8   1165      8      8\n",
      "      8      8      8      8      8      8   1166      8      8      8\n",
      "      8      8      8      8      8    492      8      8      8      8\n",
      "      8      8      8      8   7583      8      8      8      8      8\n",
      "      8      8      8    570      8      8      8      8      8      8\n",
      "      8   2036   5659   2170   7427   7672  12077   2902]\n",
      "['ICE HOCKEY-WORLD CHAMPIONSHIP STANDINGS.', 'Ice hockey world', \"championship standings after Monday's games:\", '    Pool A\\t\\t P   W   D   L   F   A  Pts', ' 1. Czech Republic     2   2   0   0   4   2   4', ' 2. Russia\\t\\t 2   1   1   0   7   3   3', ' 4. Slovakia\\t     2   1   1   0   7   5   3', ' 3. Finland\\t\\t2   1   0   1   7   3   2', ' 5. France\\t\\t 2   0   0   2   4  11   0', ' 6. Germany\\t\\t2   0   0   2   2   7   0', 'Pool B', ' 1. Sweden\\t\\t 2   2   0   0  12   5   4', ' 2. U.S.\\t\\t   2   2   0   0   8   5   4', ' 3. Canada\\t\\t 2   1   0   1   9   7   2', ' 4. Italy\\t\\t  2   1   0   1   8   9   2', ' 5. Latvia\\t\\t 2   0   0   2   8  10   0', ' 6. Norway\\t\\t 2   0   0   2   1  10   0', 'Note: Top three teams qualify for medal round']\n",
      "8\n",
      "289.834024572\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(news_train[1])\n",
    "print(news_train_t[1])\n",
    "print(word_to_index[\"NUM\"])\n",
    "lengths = np.array([len(x) for x in news_train])\n",
    "print(lengths.mean() + np.sqrt(lengths.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Work towards a deep learning solution\n",
    "from keras.preprocessing import sequence\n",
    "max_news_length = int(np.percentile(lengths, 90))\n",
    "news_train = sequence.pad_sequences(news_train, maxlen=max_news_length, padding='post', truncating='post')\n",
    "news_test = sequence.pad_sequences(news_test, maxlen=max_news_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 326)\n",
      "(10000, 326)\n",
      "[15, 16, 44]\n"
     ]
    }
   ],
   "source": [
    "print(news_train.shape)\n",
    "print(news_test.shape)\n",
    "print(list(tags_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345747    893   2029   1814      8   1812      8      8   1807   1814\n",
      "      8   4206   1804   1814      8   1814      8   1811      8      8\n",
      "   2034   2035      8   4206   1816   1191   1817   1818   1819   1760\n",
      "   1820      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "print(news_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "[15 16 44]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[116 117 123]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(n_class)\n",
    "#encode responses\n",
    "tags_train_matrix = np.zeros((n_train,n_class))\n",
    "for ii in range(n_train):\n",
    "    tags_train_matrix[ii, list(tags_train[ii])] = 1\n",
    "    \n",
    "tags_test_matrix = np.zeros((n_train,n_class))    \n",
    "for ii in range(n_test):\n",
    "    tags_test_matrix[ii, list(tags_test[ii])] = 1    \n",
    "\n",
    "print(tags_train[0])    \n",
    "print(tags_train_matrix[0,])\n",
    "\n",
    "print(tags_test[0])    \n",
    "print(tags_test_matrix[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "402715\n"
     ]
    }
   ],
   "source": [
    "print(np.array(tags_train).shape)\n",
    "print(len(word_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_index.keys())+1, p))\n",
    "for word, i in word_to_index.items():\n",
    "    vector = embeddings.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402716, 200)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[0,:])\n",
    "#print(embedding_matrix[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0487912585443\n",
      "0.123944838227\n",
      "0.487306300989\n",
      "0.803939525358\n",
      "200\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n",
      "[ -7.44000000e-01   6.62274000e-01  -6.51124000e-01   2.36011600e-01\n",
      "   5.67250000e-03  -3.07229500e-01   2.41807400e-01  -5.71931000e-01\n",
      "  -3.61158000e-01  -3.63368600e-01  -1.82348110e-01   2.38715700e-01\n",
      "  -4.82537200e-02  -1.79683700e-01   1.80469700e-01   5.38937000e-01\n",
      "  -1.50940600e-01  -3.18819000e-01  -4.94520000e-03  -2.76156900e-01\n",
      "  -1.72995400e-01   2.45195000e+00  -2.10229380e-01  -5.65326700e-01\n",
      "   1.77265086e-01   1.53283000e-01  -6.55128000e-01   1.80931700e-01\n",
      "   2.05528000e-03   5.94318000e-01   3.70452000e-01   6.87138000e-01\n",
      "  -5.74510000e-01   9.23461000e-02   2.96250000e-01   9.02266000e-02\n",
      "  -5.72540000e-01  -7.46564000e-01  -1.33162700e-01  -2.58951200e-01\n",
      "  -1.22135930e-01  -1.70256300e-01   1.17873400e-01  -5.39426000e-02\n",
      "  -1.42828820e-01   3.88476000e-01   2.41855910e-01  -3.79417600e-01\n",
      "   2.71615000e-01  -8.24286000e-02   2.86616700e-01  -3.54664000e-01\n",
      "  -6.61385000e-02   1.07114300e+00   7.36116000e-02  -3.68061000e-01\n",
      "  -5.32952000e-01   4.11150000e-01   4.01223000e-01   8.25452100e-02\n",
      "   5.33157000e-02   3.27479640e-01  -3.88304300e-02  -5.34782000e-01\n",
      "   2.09118960e-02  -4.47201000e-01   2.90662000e-01   1.34850690e-01\n",
      "   5.69142000e-01  -4.25151000e-01   2.78197000e-01  -1.15277020e-01\n",
      "  -3.24342000e-01  -7.96298000e-02   3.70169000e-01   2.82124200e-01\n",
      "  -9.88957000e-02   3.16053000e-01  -2.20569600e-01   4.70948000e-01\n",
      "  -4.69323000e-01   2.37612100e-01  -7.97103000e-02   3.23926000e-01\n",
      "  -2.92993000e-01  -5.35100000e-02   1.10292790e-01  -4.43596000e-01\n",
      "   1.34155500e-01   6.34452000e-01  -6.60382000e-01   4.17174000e-01\n",
      "   1.20768800e-01  -1.07885000e-02   6.52231000e-01  -4.87369000e-01\n",
      "  -2.72557000e-01   7.00180000e-02   3.09841000e-01  -2.97647000e-01\n",
      "   1.11580400e-01  -4.76977000e-02   2.87034000e-01  -3.60968000e-02\n",
      "  -2.32072000e-01  -3.42017000e-01   3.55159000e-01   9.10874000e-01\n",
      "   7.35639000e-02   2.68453000e-01  -4.60409000e-01   2.41079300e-01\n",
      "  -1.55528212e-01   4.64007000e-01  -3.57349000e-01  -5.34564000e-01\n",
      "   8.32303000e-02   3.16698000e-01   3.07919000e-01   1.55857320e-01\n",
      "  -1.11689400e-01   6.42818000e-02   1.83259200e-01   8.37300600e-02\n",
      "   3.91243000e-01  -1.15080000e+00   1.14522450e-01  -3.51472000e-01\n",
      "   2.78124500e-01  -4.03922000e-01  -5.50731300e-02  -7.83628000e-01\n",
      "  -1.82989900e-01  -5.63232000e-01   4.16765000e-01  -1.32374300e-01\n",
      "  -1.23644400e-01  -7.67330000e-01  -4.86432000e-01   9.64175000e-02\n",
      "  -2.16149700e-01   1.32331400e-01  -2.29091200e-02  -1.79549300e-01\n",
      "   7.54717000e-01   1.45086960e-01  -1.58735000e-01  -1.58058190e-01\n",
      "  -5.50548000e-01  -5.72892000e-02   4.71461000e-01   8.56196000e-01\n",
      "  -1.22878800e-01  -7.62552000e-02   1.23310100e-01  -2.06490430e-01\n",
      "   1.71263870e-01  -1.17530000e-01   6.11527000e-01  -9.33233000e-01\n",
      "   3.46123000e-01  -1.63162200e-01  -2.43661900e-01  -2.58017000e-01\n",
      "   2.74746300e-01   5.70981000e-01  -3.69092600e-01   6.56143000e-01\n",
      "   2.47951200e-01   4.64476000e-01   6.20601000e-01  -5.80929000e-01\n",
      "   6.17692000e-01  -6.31567940e-02  -1.17750000e-01   2.52249900e-01\n",
      "   3.74457000e-01   1.94910700e-01   1.65379500e-01  -2.81462000e-02\n",
      "   1.57017000e+00  -3.88689000e-01  -1.81958300e-01   3.93904000e-01\n",
      "  -1.87322900e-01  -6.39969000e-01  -5.52822000e-02  -9.99770300e-02\n",
      "  -2.25244200e-01  -7.28119000e-02  -5.50555000e-01   2.80025000e-01\n",
      "  -3.83015370e-01  -2.45289000e-01   6.08147600e-02   3.37175000e-01\n",
      "  -1.49961000e-01  -1.44493200e-01   7.65622000e-02  -1.80987900e-01]\n"
     ]
    }
   ],
   "source": [
    "#changing the embedding of \"NUM\" to be columnwise mean of 10 first N+\n",
    "import numpy as np\n",
    "from scipy import spatial\n",
    "from numpy.linalg import norm\n",
    "#print(embeddings[\"1\"])\n",
    "#print(embeddings[\"2\"])\n",
    "print(spatial.distance.cosine(embeddings[\"1\"], embeddings[\"2\"]))\n",
    "print(spatial.distance.cosine(embeddings[\"1\"], embeddings[\"5\"]))\n",
    "print(spatial.distance.cosine(embeddings[\"1\"], embeddings[\"one\"]))\n",
    "print(spatial.distance.cosine(embeddings[\"1\"], embeddings[\"dog\"]))\n",
    "numberss = np.zeros((10,len(embeddings[\"1\"])))\n",
    "for ii in range(10):\n",
    "    numberss[ii,:] = embeddings[str(ii)]\n",
    "    \n",
    "print(len(np.mean(numberss, axis=0)))\n",
    "print(embedding_matrix[word_to_index[\"NUM\"],:])\n",
    "embedding_matrix[word_to_index[\"NUM\"],:] = np.mean(numberss, axis=0)\n",
    "print(embedding_matrix[word_to_index[\"NUM\"],:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "#notice that this must be done without rounding, which would be in the stadard case of f1 score.\n",
    "def f1_score_loss(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.sigmoid(1000 * (K.clip(y_true * y_pred, 0, 1)) - 0.5)) #special sigmoid for imitating the rounding\n",
    "    c2 = K.sum(K.sigmoid(1000 * (K.clip(y_pred, 0, 1)) - 0.5))\n",
    "    c3 = K.sum(K.clip(y_true, 0, 1))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return -1 * f1_score #loss, we are trying to max the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import GlobalMaxPooling1D, Merge, Concatenate\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import metrics\n",
    "from sklearn.metrics import f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling\n",
    "\n",
    "We start by doing a simple LSTM layer. Motivation for this was a blog post from Jason Brownlee, [Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 326, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 126)               12726     \n",
      "=================================================================\n",
      "Total params: 80,676,326\n",
      "Trainable params: 133,126\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 165s - loss: 0.1695 - acc: 0.9652   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 164s - loss: 0.0883 - acc: 0.9743   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 164s - loss: 0.0878 - acc: 0.9743   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 164s - loss: 0.0874 - acc: 0.9743   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 167s - loss: 0.0872 - acc: 0.9744   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c276b6b38>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model with a simple LSTM layer\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=5, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.3254\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to converge to a not so good value.\n",
    "\n",
    "Next we will try out to optimize the \"f1 loss\" defined above. Note that f1 score is not differentiable, but we can approximate it by a differentiable sigmoid function with a proper scaling. A reasonable function we are using has the shift parameter of 0.5 and scale parameter of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 326, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 5)                 4120      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 512)               3072      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 80,615,030\n",
      "Trainable params: 71,830\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      "10000/10000 [==============================] - 119s - loss: -0.7672 - acc: 0.0040   \n",
      "Epoch 2/2\n",
      "10000/10000 [==============================] - 117s - loss: -0.7672 - acc: 0.0036   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c29f0b9b0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with f1 loss\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(5))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss=f1_score_loss, optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.0497\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.1 #same as we use for optimization\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the loss is getting concerningly low as the f1 score should be between 0 and 1. I have not found a bug in the f1 implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.55605\n"
     ]
    }
   ],
   "source": [
    "print(max([max(x) for x in prob_test]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score seems not to be working. The f1 score is near 0, which is the worse, and it seems that we are predicting zero class for everything. I will go back to the binary crossentropy. Next one is just basic convolutional NN. It is pretty similar to what we experimented in the exercises. The amount of parameters in the next one is pretty huge due to wide dense layer after convolutions and before the output layer. This is motivated by many successful CNN architectures as they tend to have a dense layer before output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 326, 64)           102464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 108, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 512)               3539456   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 84,249,758\n",
      "Trainable params: 84,249,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 241s - loss: 0.0889 - acc: 0.9726   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 245s - loss: 0.0441 - acc: 0.9858   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 232s - loss: 0.0332 - acc: 0.9889   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 229s - loss: 0.0251 - acc: 0.9913   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 248s - loss: 0.0189 - acc: 0.9933   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2b4cfa58>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model with a CNN layer\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 3\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=True)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=n_convolutions, kernel_size=kernel_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7876\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we get the best performance with this CNN network. Next model is an attempt to improve the performance by adding a LSTM layer. This approach is again motivated by the blog post from [Jason Brownlee](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 326, 64)           102464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 108, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 50)                23000     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               26112     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 80,759,414\n",
      "Trainable params: 216,214\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 89s - loss: 0.1523 - acc: 0.9699    \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 96s - loss: 0.0820 - acc: 0.9758    \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 88s - loss: 0.0797 - acc: 0.9761    \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 88s - loss: 0.0774 - acc: 0.9763    \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 92s - loss: 0.0744 - acc: 0.9772    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2b824550>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model with LSTM and CNN layer\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 3\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=n_convolutions, kernel_size=kernel_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.461\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN seems to be doing better job than CNN and LSTM. The iterations were faster and the convergence was reached earlier with worse performance in terms of loss. \n",
    "\n",
    "Next model was trying out different size of kernels and learning them side by side. This approach was motivated by the network graphs shown in the lectures and the implementation was learned by [this Github issue](https://github.com/fchollet/keras/issues/6547)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_1 (Merge)              (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 323,166,206\n",
      "Trainable params: 993,406\n",
      "Non-trainable params: 322,172,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 306s - loss: 0.0933 - acc: 0.9714   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 317s - loss: 0.0450 - acc: 0.9856   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 317s - loss: 0.0351 - acc: 0.9883   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 311s - loss: 0.0285 - acc: 0.9900   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 312s - loss: 0.0237 - acc: 0.9916   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2bea5f28>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model with multiple size kernels\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 3\n",
    "kernels = (3,5,8,10)\n",
    "n_filters = 128\n",
    "\n",
    "submodels = []\n",
    "for kw in kernels:    # kernel sizes\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False))\n",
    "    submodel.add(Conv1D(n_filters,\n",
    "                        kw,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        strides=1))\n",
    "    submodel.add(GlobalMaxPooling1D())\n",
    "    submodels.append(submodel)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Merge(submodels, mode=\"concat\"))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "news_train_rep = [np.array(news_train)] * len(kernels)\n",
    "\n",
    "model.fit(news_train_rep, np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.775\n"
     ]
    }
   ],
   "source": [
    "news_test_rep = [np.array(news_test)] * len(kernels)\n",
    "prob_test = model.predict(news_test_rep, batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to converge fast, but the final result is still in the same level as everything else. We can try to boost this with an LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:27: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer max_pooling1d_3: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-8de116ff2d49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooling_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;31m#model.add(Flatten())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    473\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    474\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer max_pooling1d_3: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# create a model with multiple size kernels and LSTM\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 5\n",
    "kernels = (3,5,8,10)\n",
    "n_filters = 128\n",
    "\n",
    "submodels = []\n",
    "for kw in kernels:    # kernel sizes\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False))\n",
    "    submodel.add(Conv1D(n_filters,\n",
    "                        kw,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        strides=1))\n",
    "    submodel.add(GlobalMaxPooling1D())\n",
    "    submodels.append(submodel)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Merge(submodels, mode=\"concat\"))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "#model.add(Flatten())\n",
    "print(model.summary())\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "news_train_rep = [np.array(news_train)] * len(kernels)\n",
    "\n",
    "model.fit(news_train_rep, np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test = model.predict(news_test_rep, batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension mismatch somewhere? I could not solve what is wrong with the above approach. However a reasonable idea.\n",
    "\n",
    "Next approach we took was the Bidirectional LSTM. It is used for a sequence classification problems where the whole sequence is known. So for regular time series it would not be applicable, but for sequence classifications such as an article it is ok. The motivation for this came through serious information retrieval from Jason Bronwlee's blog post [How to Develop a Bidirectional LSTM For Sequence Classification in Python with Keras](https://machinelearningmastery.com/develop-bidirectional-lstm-sequence-classification-python-keras/) and from Richard Liao's blog [Text Classification, Part 2 - sentence level Attentional RNN](https://richliao.github.io/supervised/classification/2016/12/26/textclassifier-RNN/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 326, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 80,951,550\n",
      "Trainable params: 408,350\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 311s - loss: 0.1151 - acc: 0.9688   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 298s - loss: 0.0573 - acc: 0.9822   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 295s - loss: 0.0458 - acc: 0.9848   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 294s - loss: 0.0397 - acc: 0.9865   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 295s - loss: 0.0354 - acc: 0.9879   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2c469b00>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bidirectional LSTM\n",
    "from keras.layers import Bidirectional\n",
    "epochs=5\n",
    "batch_size=64\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(n_class, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7067\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 326, 64)           64064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 108, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 108, 128)          41088     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 36, 128)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 200)               183200    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 80,999,102\n",
      "Trainable params: 455,902\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 72s - loss: 0.1116 - acc: 0.9677    \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 69s - loss: 0.0761 - acc: 0.9768    \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 69s - loss: 0.0578 - acc: 0.9818    \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 69s - loss: 0.0476 - acc: 0.9843    \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 69s - loss: 0.0403 - acc: 0.9863    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2dae40f0>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#bidirectional LSTM with couple of CNN layers\n",
    "from keras.layers import Bidirectional\n",
    "epochs=5\n",
    "batch_size=64\n",
    "n_convolution = 128\n",
    "kernel_size = 5\n",
    "pooling_size=3\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=n_convolutions, kernel_size=kernel_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(Conv1D(filters=2*n_convolutions, kernel_size=kernel_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(n_class, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.6779\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly bad, even though I got a good results with a bidirectional LSTM and CNNs separately. Also our group has been discussing that CNN is the way to go because of their good results. Next one is a modification of the model that we are looking to try out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 323, 300)          240300    \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 318, 100)          180100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 106, 100)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 97, 100)           100100    \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 9700)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 512)               4966912   \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 86,095,250\n",
      "Trainable params: 5,552,050\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 197s - loss: 0.0924 - acc: 0.9718   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 193s - loss: 0.0500 - acc: 0.9843   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 203s - loss: 0.0405 - acc: 0.9867   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 202s - loss: 0.0343 - acc: 0.9884   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 193s - loss: 0.0290 - acc: 0.9899   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c30ca5cc0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Anisia super model\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(300, 4, activation='relu'))\n",
    "model.add(Conv1D(100, 6, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Conv1D(100, 10, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7587\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach did not achieve such great results here, but in the larger data set it achieved near 0.85 f1 score. I guess the final evaluation will show how good our models were.\n",
    "\n",
    "I finally want to check some baseline scores when predicting exactly correct, only ones and only zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  1.0\n",
      "F1 score:  0.0542\n",
      "F1 score:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print('F1 score: ', round(f1_score(pred_test, pred_test, average='micro'), 4))\n",
    "print('F1 score: ', round(f1_score(np.ones(pred_test.shape), pred_test, average='micro'), 4))\n",
    "print('F1 score: ', round(f1_score(np.zeros(pred_test.shape), pred_test, average='micro'), 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the target variable into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(topics)\n",
    "y_train = mlb.fit_transform(tags_train)\n",
    "y_test = mlb.fit_transform(tags_test)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape, '\\n')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data set (this is just for test)\n",
    "Then we will convert the training and test sets into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import itertools\n",
    "\n",
    "max_vocabulary = 30000 # take only max_vocabulary most popular words\n",
    "tokenizer = Tokenizer(max_vocabulary)\n",
    "\n",
    "# concatenate each news item into a single string\n",
    "words_train = [' '.join(filter(None, news_item)) for news_item in news_train] \n",
    "tokenizer.fit_on_texts(words_train)\n",
    "matrix_train = tokenizer.texts_to_matrix(words_train)\n",
    "\n",
    "words_test = [' '.join(filter(None, news_item)) for news_item in news_test] \n",
    "matrix_test = tokenizer.texts_to_matrix(words_test)\n",
    "\n",
    "print(matrix_train.shape)\n",
    "print(matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the F1 score that is our error metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "Okay, finally we can define a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_vocabulary,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training for some iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(matrix_train, \n",
    "                    y_train, \n",
    "                    epochs=5, \n",
    "                    batch_size=128,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_test = model.predict(matrix_test, batch_size=128)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(y_test, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for the first point of test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_test[0])\n",
    "print(pred_test[0])\n",
    "print(prob_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "Finally, save your best model to the competition and return it as an `h5` file. For example like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` (e.g., by calling `y=model.predict(x_test)`) you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA20001 Deep Learning - Group Project\n",
    "## Text project\n",
    "\n",
    "**Due Wednesday December 13, before 23:59.**\n",
    "\n",
    "The task is to learn to assign the correct labels to news articles.  The corpus contains ~850K articles from Reuters.  The test set is about 10% of the articles. The data is unextracted in XML files.\n",
    "\n",
    "We're only giving you the code for downloading the data, and how to save the final model. The rest you'll have to do yourselves.\n",
    "\n",
    "Some comments and hints particular to the project:\n",
    "\n",
    "- One document may belong to many classes in this problem, i.e., it's a multi-label classification problem. In fact there are documents that don't belong to any class, and you should also be able to handle these correctly. Pay careful attention to how you design the outputs of the network (e.g., what activation to use) and what loss function should be used.\n",
    "- You may use word-embeddings to get better results. For example, you were already using a smaller version of the GloVE  embeddings in exercise 4. Do note that these embeddings take a lot of memory, but if you use the keras.embedding layer, it will be more efficient. \n",
    "- Loading all documents into one big matrix as we have done in the exercises is not feasible (e.g. the virtual servers in CSC have only 3 GB of RAM). You need to load the documents in smaller chunks for the training. This shouldn't be a problem, as we are doing mini-batch training anyway, and thus we don't need to keep all the documents in memory. You can simply pass you current chunk of documents to `model.fit()` as it remembers the weights from the previous run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data\n",
    "Let's first set some paths & download the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set already downloaded.\n",
      "Data set already unzipped.\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import download_data \n",
    "\n",
    "database_path = 'train/'\n",
    "corpus_path = database_path + 'REUTERS_CORPUS_2/'\n",
    "data_path = corpus_path + 'data/'\n",
    "codes_path = corpus_path + 'codes/'\n",
    "\n",
    "download_data(database_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above command downloads and extracts the data files into the `train` subdirectory.\n",
    "\n",
    "The files can be found in `train/`, and are named as `19970405.zip`, etc. You will have to manage the content of these zips to get the data. There is a readme which has links to further descriptions on the data.\n",
    "\n",
    "The class labels, or topics, can be found in the readme file called `train/codes.zip`.  The zip contains a file called \"topic_codes.txt\".  This file contains the special codes for the topics (about 130 of them), and the explanation - what each code means.  \n",
    "\n",
    "The XML document files contain the article's headline, the main body text, and the list of topic labels assigned to each article.  You will have to extract the topics of each article from the XML.  For example: \n",
    "&lt;code code=\"C18\"&gt; refers to the topic \"OWNERSHIP CHANGES\" (like a corporate buyout).\n",
    "\n",
    "You should pre-process the XML to extract the words from the article: the &lt;headline&gt; element and the &lt;text&gt;.  You should not need any other parts of the article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data set\n",
    "First we will read the codes into the dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126  different classes\n",
      "\n",
      "\n",
      "['1POL', '2ECO', '3SPO', '4GEN', '6INS', '7RSK', '8YDB', '9BNX', 'ADS10', 'BNW14', 'BRP11', 'C11', 'C12', 'C13', 'C14', 'C15', 'C151', 'C1511', 'C152', 'C16', 'C17', 'C171', 'C172', 'C173', 'C174', 'C18', 'C181', 'C182', 'C183', 'C21', 'C22', 'C23', 'C24', 'C31', 'C311', 'C312', 'C313', 'C32', 'C33', 'C331', 'C34', 'C41', 'C411', 'C42', 'CCAT', 'E11', 'E12', 'E121', 'E13', 'E131', 'E132', 'E14', 'E141', 'E142', 'E143', 'E21', 'E211', 'E212', 'E31', 'E311', 'E312', 'E313', 'E41', 'E411', 'E51', 'E511', 'E512', 'E513', 'E61', 'E71', 'ECAT', 'ENT12', 'G11', 'G111', 'G112', 'G113', 'G12', 'G13', 'G131', 'G14', 'G15', 'G151', 'G152', 'G153', 'G154', 'G155', 'G156', 'G157', 'G158', 'G159', 'GCAT', 'GCRIM', 'GDEF', 'GDIP', 'GDIS', 'GEDU', 'GENT', 'GENV', 'GFAS', 'GHEA', 'GJOB', 'GMIL', 'GOBIT', 'GODD', 'GPOL', 'GPRO', 'GREL', 'GSCI', 'GSPO', 'GTOUR', 'GVIO', 'GVOTE', 'GWEA', 'GWELF', 'M11', 'M12', 'M13', 'M131', 'M132', 'M14', 'M141', 'M142', 'M143', 'MCAT', 'MEUR', 'PRB13'] \n",
      "\n",
      "\n",
      "{'1POL': 0, '2ECO': 1, '3SPO': 2, '4GEN': 3, '6INS': 4, '7RSK': 5, '8YDB': 6, '9BNX': 7, 'ADS10': 8, 'BNW14': 9, 'BRP11': 10, 'C11': 11, 'C12': 12, 'C13': 13, 'C14': 14, 'C15': 15, 'C151': 16, 'C1511': 17, 'C152': 18, 'C16': 19, 'C17': 20, 'C171': 21, 'C172': 22, 'C173': 23, 'C174': 24, 'C18': 25, 'C181': 26, 'C182': 27, 'C183': 28, 'C21': 29, 'C22': 30, 'C23': 31, 'C24': 32, 'C31': 33, 'C311': 34, 'C312': 35, 'C313': 36, 'C32': 37, 'C33': 38, 'C331': 39, 'C34': 40, 'C41': 41, 'C411': 42, 'C42': 43, 'CCAT': 44, 'E11': 45, 'E12': 46, 'E121': 47, 'E13': 48, 'E131': 49, 'E132': 50, 'E14': 51, 'E141': 52, 'E142': 53, 'E143': 54, 'E21': 55, 'E211': 56, 'E212': 57, 'E31': 58, 'E311': 59, 'E312': 60, 'E313': 61, 'E41': 62, 'E411': 63, 'E51': 64, 'E511': 65, 'E512': 66, 'E513': 67, 'E61': 68, 'E71': 69, 'ECAT': 70, 'ENT12': 71, 'G11': 72, 'G111': 73, 'G112': 74, 'G113': 75, 'G12': 76, 'G13': 77, 'G131': 78, 'G14': 79, 'G15': 80, 'G151': 81, 'G152': 82, 'G153': 83, 'G154': 84, 'G155': 85, 'G156': 86, 'G157': 87, 'G158': 88, 'G159': 89, 'GCAT': 90, 'GCRIM': 91, 'GDEF': 92, 'GDIP': 93, 'GDIS': 94, 'GEDU': 95, 'GENT': 96, 'GENV': 97, 'GFAS': 98, 'GHEA': 99, 'GJOB': 100, 'GMIL': 101, 'GOBIT': 102, 'GODD': 103, 'GPOL': 104, 'GPRO': 105, 'GREL': 106, 'GSCI': 107, 'GSPO': 108, 'GTOUR': 109, 'GVIO': 110, 'GVOTE': 111, 'GWEA': 112, 'GWELF': 113, 'M11': 114, 'M12': 115, 'M13': 116, 'M131': 117, 'M132': 118, 'M14': 119, 'M141': 120, 'M142': 121, 'M143': 122, 'MCAT': 123, 'MEUR': 124, 'PRB13': 125} \n",
      "\n",
      "\n",
      "1POL  :  CURRENT NEWS - POLITICS\n",
      "2ECO  :  CURRENT NEWS - ECONOMICS\n",
      "3SPO  :  CURRENT NEWS - SPORT\n",
      "4GEN  :  CURRENT NEWS - GENERAL\n",
      "6INS  :  CURRENT NEWS - INSURANCE\n",
      "7RSK  :  CURRENT NEWS - RISK NEWS\n",
      "8YDB  :  TEMPORARY\n",
      "9BNX  :  TEMPORARY\n",
      "ADS10  :  CURRENT NEWS - ADVERTISING\n",
      "BNW14  :  CURRENT NEWS - BUSINESS NEWS\n",
      "BRP11  :  CURRENT NEWS - BRANDS\n",
      "C11  :  STRATEGY/PLANS\n",
      "C12  :  LEGAL/JUDICIAL\n",
      "C13  :  REGULATION/POLICY\n",
      "C14  :  SHARE LISTINGS\n",
      "C15  :  PERFORMANCE\n",
      "C151  :  ACCOUNTS/EARNINGS\n",
      "C1511  :  ANNUAL RESULTS\n",
      "C152  :  COMMENT/FORECASTS\n",
      "C16  :  INSOLVENCY/LIQUIDITY\n",
      "C17  :  FUNDING/CAPITAL\n",
      "C171  :  SHARE CAPITAL\n",
      "C172  :  BONDS/DEBT ISSUES\n",
      "C173  :  LOANS/CREDITS\n",
      "C174  :  CREDIT RATINGS\n",
      "C18  :  OWNERSHIP CHANGES\n",
      "C181  :  MERGERS/ACQUISITIONS\n",
      "C182  :  ASSET TRANSFERS\n",
      "C183  :  PRIVATISATIONS\n",
      "C21  :  PRODUCTION/SERVICES\n",
      "C22  :  NEW PRODUCTS/SERVICES\n",
      "C23  :  RESEARCH/DEVELOPMENT\n",
      "C24  :  CAPACITY/FACILITIES\n",
      "C31  :  MARKETS/MARKETING\n",
      "C311  :  DOMESTIC MARKETS\n",
      "C312  :  EXTERNAL MARKETS\n",
      "C313  :  MARKET SHARE\n",
      "C32  :  ADVERTISING/PROMOTION\n",
      "C33  :  CONTRACTS/ORDERS\n",
      "C331  :  DEFENCE CONTRACTS\n",
      "C34  :  MONOPOLIES/COMPETITION\n",
      "C41  :  MANAGEMENT\n",
      "C411  :  MANAGEMENT MOVES\n",
      "C42  :  LABOUR\n",
      "CCAT  :  CORPORATE/INDUSTRIAL\n",
      "E11  :  ECONOMIC PERFORMANCE\n",
      "E12  :  MONETARY/ECONOMIC\n",
      "E121  :  MONEY SUPPLY\n",
      "E13  :  INFLATION/PRICES\n",
      "E131  :  CONSUMER PRICES\n",
      "E132  :  WHOLESALE PRICES\n",
      "E14  :  CONSUMER FINANCE\n",
      "E141  :  PERSONAL INCOME\n",
      "E142  :  CONSUMER CREDIT\n",
      "E143  :  RETAIL SALES\n",
      "E21  :  GOVERNMENT FINANCE\n",
      "E211  :  EXPENDITURE/REVENUE\n",
      "E212  :  GOVERNMENT BORROWING\n",
      "E31  :  OUTPUT/CAPACITY\n",
      "E311  :  INDUSTRIAL PRODUCTION\n",
      "E312  :  CAPACITY UTILIZATION\n",
      "E313  :  INVENTORIES\n",
      "E41  :  EMPLOYMENT/LABOUR\n",
      "E411  :  UNEMPLOYMENT\n",
      "E51  :  TRADE/RESERVES\n",
      "E511  :  BALANCE OF PAYMENTS\n",
      "E512  :  MERCHANDISE TRADE\n",
      "E513  :  RESERVES\n",
      "E61  :  HOUSING STARTS\n",
      "E71  :  LEADING INDICATORS\n",
      "ECAT  :  ECONOMICS\n",
      "ENT12  :  CURRENT NEWS - ENTERTAINMENT\n",
      "G11  :  SOCIAL AFFAIRS\n",
      "G111  :  HEALTH/SAFETY\n",
      "G112  :  SOCIAL SECURITY\n",
      "G113  :  EDUCATION/RESEARCH\n",
      "G12  :  INTERNAL POLITICS\n",
      "G13  :  INTERNATIONAL RELATIONS\n",
      "G131  :  DEFENCE\n",
      "G14  :  ENVIRONMENT\n",
      "G15  :  EUROPEAN COMMUNITY\n",
      "G151  :  EC INTERNAL MARKET\n",
      "G152  :  EC CORPORATE POLICY\n",
      "G153  :  EC AGRICULTURE POLICY\n",
      "G154  :  EC MONETARY/ECONOMIC\n",
      "G155  :  EC INSTITUTIONS\n",
      "G156  :  EC ENVIRONMENT ISSUES\n",
      "G157  :  EC COMPETITION/SUBSIDY\n",
      "G158  :  EC EXTERNAL RELATIONS\n",
      "G159  :  EC GENERAL\n",
      "GCAT  :  GOVERNMENT/SOCIAL\n",
      "GCRIM  :  CRIME, LAW ENFORCEMENT\n",
      "GDEF  :  DEFENCE\n",
      "GDIP  :  INTERNATIONAL RELATIONS\n",
      "GDIS  :  DISASTERS AND ACCIDENTS\n",
      "GEDU  :  EDUCATION\n",
      "GENT  :  ARTS, CULTURE, ENTERTAINMENT\n",
      "GENV  :  ENVIRONMENT AND NATURAL WORLD\n",
      "GFAS  :  FASHION\n",
      "GHEA  :  HEALTH\n",
      "GJOB  :  LABOUR ISSUES\n",
      "GMIL  :  MILLENNIUM ISSUES\n",
      "GOBIT  :  OBITUARIES\n",
      "GODD  :  HUMAN INTEREST\n",
      "GPOL  :  DOMESTIC POLITICS\n",
      "GPRO  :  BIOGRAPHIES, PERSONALITIES, PEOPLE\n",
      "GREL  :  RELIGION\n",
      "GSCI  :  SCIENCE AND TECHNOLOGY\n",
      "GSPO  :  SPORTS\n",
      "GTOUR  :  TRAVEL AND TOURISM\n",
      "GVIO  :  WAR, CIVIL WAR\n",
      "GVOTE  :  ELECTIONS\n",
      "GWEA  :  WEATHER\n",
      "GWELF  :  WELFARE, SOCIAL SERVICES\n",
      "M11  :  EQUITY MARKETS\n",
      "M12  :  BOND MARKETS\n",
      "M13  :  MONEY MARKETS\n",
      "M131  :  INTERBANK MARKETS\n",
      "M132  :  FOREX MARKETS\n",
      "M14  :  COMMODITY MARKETS\n",
      "M141  :  SOFT COMMODITIES\n",
      "M142  :  METALS TRADING\n",
      "M143  :  ENERGY MARKETS\n",
      "MCAT  :  MARKETS\n",
      "MEUR  :  EURO CURRENCY\n",
      "PRB13  :  CURRENT NEWS - PRESS RELEASE WIRES\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import read_topics\n",
    "\n",
    "(topics, topic_index, topic_labels) = read_topics(database_path)\n",
    "n_class = len(topics)\n",
    "\n",
    "print(n_class, ' different classes\\n\\n')\n",
    "print(topics, '\\n\\n')           # topics codes as an array\n",
    "print(topic_index, '\\n\\n')      # dictionary of topic code : index of this code in \"topics array\"\n",
    "\n",
    "for key in topic_labels:        # dictionary of topic code : label of this topic code\n",
    "    print(key, ' : ', topic_labels[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read a small training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['C15', 'C151', 'CCAT'], ['GCAT', 'GSPO'], ['E12', 'ECAT', 'M13', 'M132', 'MCAT']] \n",
      "\n",
      "['TABLE-Primus Telecommunications Q2 loss.', '3 Months', '\\t\\t\\t     1997\\t\\t\\t1996', ' Shr  loss\\t\\t   $0.50\\t\\t\\t NA', ' Net  loss\\t\\t  $9,000     loss\\t$8,900', ' Revs\\t\\t\\t$70,000\\t\\t  $59,000', ' Avg shrs\\t     17,778,731\\t\\t\\t NA', '(All data above 000s except per share numbers)'] \n",
      "\n",
      "[['M13', 'M131', 'MCAT'], ['M13', 'M131', 'MCAT'], ['C15', 'C151', 'CCAT']] \n",
      "\n",
      "['Canadian T-bills open mostly weaker in quiet trade.', 'Canadian T-bills opened mostly weaker in quiet trade on Tuesday, taking much of their tone from U.S. Treasuries as dealers returned to the market following a long weekend in most of Canada.', '\"I\\'ve seen a couple of sellers in the front end, nothing too huge though,\" said one T-bill dealer with a bank-owned brokerage. \"I think that the Canadian money market guys are going to start to play this thing more cautiously.\"', \"Canada's three-month cash T-bill softened to yield 3.28 percent from 3.27 percent from the close of trading on Friday.  \", \"Dealers said the money market took its direction from U.S. Treasuries, which weakened on Monday while fixed-income markets were closed in Ontario. Most money market trading takes place in Canada's most populous province.\", 'Looking ahead, one dealer said the medium- to long-term view remains bearish for Canadian T-bills, with the possibility of a second Bank of Canada rate hike keeping prices from advancing very far.', 'In other prices, the six-month cash T-bill softened to yield 3.62 percent from 3.61 percent on Friday. The one-year when-issued T-bill traded unchanged to yield 4.17 percent.  ', \"The call loan or overnight lending rate traded at 3.35 percent, in the upper half of the Bank of Canada's 3.0-3.5 percent target range.\", '((Jeffrey Hodgson (416) 941-8105, e-mail: jeffrey.hodgson@reuters.com))'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import read_news\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 10000\n",
    "\n",
    "(news_train, tags_train, news_test, tags_test) = read_news(database_path, n_train, n_test, seed = 1234)\n",
    "\n",
    "print(tags_train[0:3], '\\n')\n",
    "print(news_train[0], '\\n')\n",
    "\n",
    "print(tags_test[0:3], '\\n')\n",
    "print(news_test[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe Zip found\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import download_glove\n",
    "embeddings_path = \"embeddings/\"\n",
    "download_glove(embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already unzipped\n"
     ]
    }
   ],
   "source": [
    "from src.data_utility import unzip_glove\n",
    "zip_file_name = \"glove.6B.zip\"\n",
    "unzip_glove(embeddings_path, zip_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import get_glove_embeddings\n",
    "p=200\n",
    "embeddings = get_glove_embeddings(p, embeddings_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "400000\n"
     ]
    }
   ],
   "source": [
    "print(len(embeddings[\"the\"]))\n",
    "print(len(embeddings.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import process_data\n",
    "if not os.path.exists(\"train/REUTERS_CORPUS_2/tokenized/\"): process_data(\"train/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import build_dictionary\n",
    "if not os.path.exists(\"dictionary.json\"): build_dictionary(\"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "word_to_index = json.loads(open(\"dictionary.json\").read())\n",
    "dict_size = len(word_to_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import vectorize_data\n",
    "if not os.path.exists(\"train/REUTERS_CORPUS_2/vectorized/\"): vectorize_data(\"train/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from src.data_utility import get_vectorized_data\n",
    "vectorized_data_path = \"train/REUTERS_CORPUS_2/vectorized/\"\n",
    "tags_path=\"train/REUTERS_CORPUS_2/tags/\"\n",
    "n_train=10000\n",
    "n_test=10000\n",
    "(news_train, tags_train, news_test, tags_test) = get_vectorized_data(vectorized_data_path, tags_path, n_train, n_test, seed = 1234)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  9263 100301   7404   7511   9263   9294    460   7404   7511   3560\n",
      "   4090   9041   1242   8212   4484   5633   1344   2939   1242   4595\n",
      "      8   7597   7598      8      8      8      8      8      8      8\n",
      "      8   7855      8      8      8      8      8      8      8      8\n",
      "   7603      8      8      8      8      8      8      8      8   6701\n",
      "      8      8      8      8      8      8      8      8   1167      8\n",
      "      8      8      8      8      8      8      8   1168      8      8\n",
      "      8      8      8      8      8   9041    764      8   7582      8\n",
      "      8      8      8      8      8      8      8   1165      8      8\n",
      "      8      8      8      8      8      8   1166      8      8      8\n",
      "      8      8      8      8      8    492      8      8      8      8\n",
      "      8      8      8      8   7583      8      8      8      8      8\n",
      "      8      8      8    570      8      8      8      8      8      8\n",
      "      8   2036   5659   2170   7427   7672  12077   2902]\n",
      "8\n",
      "289.834024572\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(news_train[1])\n",
    "print(word_to_index[\"NUM\"])\n",
    "lengths = np.array([len(x) for x in news_train])\n",
    "print(lengths.mean() + np.sqrt(lengths.var()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Work towards LSTM solution\n",
    "from keras.preprocessing import sequence\n",
    "max_news_length = int(np.percentile(lengths, 90))\n",
    "news_train = sequence.pad_sequences(news_train, maxlen=max_news_length, padding='post', truncating='post')\n",
    "news_test = sequence.pad_sequences(news_test, maxlen=max_news_length, padding='post', truncating='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 326)\n",
      "(10000, 326)\n",
      "[15, 16, 44]\n"
     ]
    }
   ],
   "source": [
    "print(news_train.shape)\n",
    "print(news_test.shape)\n",
    "print(list(tags_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[345747    893   2029   1814      8   1812      8      8   1807   1814\n",
      "      8   4206   1804   1814      8   1814      8   1811      8      8\n",
      "   2034   2035      8   4206   1816   1191   1817   1818   1819   1760\n",
      "   1820      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0]\n"
     ]
    }
   ],
   "source": [
    "print(news_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n",
      "[15 16 44]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[116 117 123]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(n_class)\n",
    "#encode responses\n",
    "tags_train_matrix = np.zeros((n_train,n_class))\n",
    "for ii in range(n_train):\n",
    "    tags_train_matrix[ii, list(tags_train[ii])] = 1\n",
    "    \n",
    "tags_test_matrix = np.zeros((n_train,n_class))    \n",
    "for ii in range(n_test):\n",
    "    tags_test_matrix[ii, list(tags_test[ii])] = 1    \n",
    "\n",
    "print(tags_train[0])    \n",
    "print(tags_train_matrix[0,])\n",
    "\n",
    "print(tags_test[0])    \n",
    "print(tags_test_matrix[0,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "402715\n"
     ]
    }
   ],
   "source": [
    "print(np.array(tags_train).shape)\n",
    "print(len(word_to_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((len(word_to_index.keys())+1, p))\n",
    "for word, i in word_to_index.items():\n",
    "    vector = embeddings.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402716, 200)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.shape)\n",
    "print(embedding_matrix[0,:])\n",
    "#print(embedding_matrix[1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "#notice that this must be done without rounding, which would be in the stadard case of f1 score.\n",
    "def f1_score_own(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.sigmoid(10 * (K.clip(y_true * y_pred, 0, 1)) - 0.5)) #special sigmoid for imitating the rounding\n",
    "    c2 = K.sum(K.sigmoid(10 * (K.clip(y_pred, 0, 1)) - 0.5))\n",
    "    c3 = K.sum(K.clip(y_true, 0, 1))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return -1 * f1_score #loss, we are trying to max the f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 326, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               120400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 126)               12726     \n",
      "=================================================================\n",
      "Total params: 80,676,326\n",
      "Trainable params: 133,126\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 184s - loss: 0.1658 - acc: 0.9639   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 179s - loss: 0.0883 - acc: 0.9743   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 177s - loss: 0.0879 - acc: 0.9743   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 172s - loss: 0.0874 - acc: 0.9744   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 177s - loss: 0.0872 - acc: 0.9744   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1be67c88>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model with a simple LSTM layer\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=5, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.3372\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to get stuck or converge at very bad (in terms of performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 326, 200)          0         \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 5)                 4120      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 512)               3072      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 80,615,030\n",
      "Trainable params: 71,830\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 117s - loss: -1.5266 - acc: 0.0023   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 115s - loss: -1.8735 - acc: 0.0019   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 115s - loss: -1.8737 - acc: 0.0019   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 118s - loss: -1.8737 - acc: 0.0019   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 118s - loss: -1.8737 - acc: 0.0019   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c24c91080>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model with f1 loss\n",
    "batch_size = 64\n",
    "epochs = 2\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Dropout(0.1))\n",
    "model.add(LSTM(5))\n",
    "model.add(Dense(512))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss=f1_score_own, optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.0\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score seems not to be working. :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 326, 64)           102464    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 108, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 6912)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               3539456   \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 84,249,758\n",
      "Trainable params: 84,249,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 265s - loss: 0.0890 - acc: 0.9729   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 251s - loss: 0.0435 - acc: 0.9860   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 243s - loss: 0.0314 - acc: 0.9893   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 240s - loss: 0.0236 - acc: 0.9918   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 242s - loss: 0.0178 - acc: 0.9937   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2548cf98>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Flatten\n",
    "# create the model with a CNN layer\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 3\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=True)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=n_convolutions, kernel_size=kernel_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7969\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 326, 200)          80543200  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 326, 32)           19232     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 108, 32)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 50)                16600     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 512)               26112     \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 80,669,782\n",
      "Trainable params: 126,582\n",
      "Non-trainable params: 80,543,200\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 61s - loss: 0.1618 - acc: 0.9698    \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 56s - loss: 0.0861 - acc: 0.9747    \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 58s - loss: 0.0811 - acc: 0.9757    \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 58s - loss: 0.0777 - acc: 0.9764    \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 58s - loss: 0.0749 - acc: 0.9767    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c257f36a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the model with LSTM and CNN layer\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 32\n",
    "kernel_size = 3\n",
    "pooling_size = 3\n",
    "model = Sequential()\n",
    "\n",
    "embedding_layer = Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False)\n",
    "\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=n_convolutions, kernel_size=kernel_size, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(LSTM(50, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "model.fit(np.array(news_train), np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.4178\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(np.array(news_test), batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN seems to be doing better job than CNN and LSTM. However based on test adding the Dense layer with a lot of hidden nodes actually have the highest impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_17 (Merge)             (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 126)               64638     \n",
      "=================================================================\n",
      "Total params: 323,166,206\n",
      "Trainable params: 993,406\n",
      "Non-trainable params: 322,172,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 308s - loss: 0.0913 - acc: 0.9722   \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 311s - loss: 0.0442 - acc: 0.9859   \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 308s - loss: 0.0340 - acc: 0.9885   \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 317s - loss: 0.0277 - acc: 0.9904   \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 307s - loss: 0.0226 - acc: 0.9921   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c26d8e390>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a model with multiple size kernels\n",
    "from keras.layers import GlobalMaxPooling1D, Merge, Concatenate\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 3\n",
    "kernels = (3,5,8,10)\n",
    "n_filters = 128\n",
    "\n",
    "submodels = []\n",
    "for kw in kernels:    # kernel sizes\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False))\n",
    "    submodel.add(Conv1D(n_filters,\n",
    "                        kw,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        strides=1))\n",
    "    submodel.add(GlobalMaxPooling1D())\n",
    "    submodels.append(submodel)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Merge(submodels, mode=\"concat\"))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "news_train_rep = [np.array(news_train)] * len(kernels)\n",
    "\n",
    "model.fit(news_train_rep, np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7764\n"
     ]
    }
   ],
   "source": [
    "news_test_rep = [np.array(news_test)] * len(kernels)\n",
    "prob_test = model.predict(news_test_rep, batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to converge fast, but the final edge is still as bad as everything else. We can try to boost this with an LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:28: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 is incompatible with layer max_pooling1d_7: expected ndim=3, found ndim=2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-c8df11f22462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"concat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooling_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    473\u001b[0m                           output_shapes=[self.outputs[0]._keras_shape])\n\u001b[1;32m    474\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m             \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 raise TypeError('All layers in a Sequential model '\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    455\u001b[0m                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m                                      str(K.ndim(x)))\n\u001b[0m\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m                 \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 is incompatible with layer max_pooling1d_7: expected ndim=3, found ndim=2"
     ]
    }
   ],
   "source": [
    "# create a model with multiple size kernels and LSTM\n",
    "from keras.layers import GlobalMaxPooling1D, Merge, Concatenate\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "n_convolutions = 64\n",
    "kernel_size = 8\n",
    "pooling_size = 5\n",
    "kernels = (3,5,8,10)\n",
    "n_filters = 128\n",
    "\n",
    "submodels = []\n",
    "for kw in kernels:    # kernel sizes\n",
    "    submodel = Sequential()\n",
    "    submodel.add(Embedding(len(word_to_index.keys())+1,\n",
    "                                p,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_news_length,\n",
    "                                trainable=False))\n",
    "    submodel.add(Conv1D(n_filters,\n",
    "                        kw,\n",
    "                        padding='valid',\n",
    "                        activation='relu',\n",
    "                        strides=1))\n",
    "    submodel.add(GlobalMaxPooling1D())\n",
    "    submodels.append(submodel)\n",
    "    \n",
    "model = Sequential()\n",
    "model.add(Merge(submodels, mode=\"concat\"))\n",
    "model.add(MaxPooling1D(pool_size=pooling_size))\n",
    "model.add(Flatten())\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dense(512, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(n_class, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "print(model.summary())\n",
    "\n",
    "news_train_rep = [np.array(news_train)] * len(kernels)\n",
    "\n",
    "model.fit(news_train_rep, np.array(tags_train_matrix),epochs=epochs, batch_size=batch_size)\n",
    "#validation_data=(np.array(news_test), np.array(tags_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7764\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(news_test_rep, batch_size=batch_size)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(tags_test_matrix, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the target variable into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 126)\n",
      "(10000, 126) \n",
      "\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(topics)\n",
    "y_train = mlb.fit_transform(tags_train)\n",
    "y_test = mlb.fit_transform(tags_test)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape, '\\n')\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data set (this is just for test)\n",
    "Then we will convert the training and test sets into one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 30000)\n",
      "(10000, 30000)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import itertools\n",
    "\n",
    "max_vocabulary = 30000 # take only max_vocabulary most popular words\n",
    "tokenizer = Tokenizer(max_vocabulary)\n",
    "\n",
    "# concatenate each news item into a single string\n",
    "words_train = [' '.join(filter(None, news_item)) for news_item in news_train] \n",
    "tokenizer.fit_on_texts(words_train)\n",
    "matrix_train = tokenizer.texts_to_matrix(words_train)\n",
    "\n",
    "words_test = [' '.join(filter(None, news_item)) for news_item in news_test] \n",
    "matrix_test = tokenizer.texts_to_matrix(words_test)\n",
    "\n",
    "print(matrix_train.shape)\n",
    "print(matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import the F1 score that is our error metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model\n",
    "Okay, finally we can define a simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               15360512  \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 126)               64638     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 126)               0         \n",
      "=================================================================\n",
      "Total params: 15,425,150\n",
      "Trainable params: 15,425,150\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_vocabulary,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(y_train.shape[1]))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try training for some iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "10000/10000 [==============================] - 29s - loss: 0.1488 - acc: 0.9614    \n",
      "Epoch 2/5\n",
      "10000/10000 [==============================] - 30s - loss: 0.0502 - acc: 0.9860    \n",
      "Epoch 3/5\n",
      "10000/10000 [==============================] - 28s - loss: 0.0328 - acc: 0.9897    \n",
      "Epoch 4/5\n",
      "10000/10000 [==============================] - 27s - loss: 0.0232 - acc: 0.9925    \n",
      "Epoch 5/5\n",
      "10000/10000 [==============================] - 27s - loss: 0.0175 - acc: 0.9943    \n",
      "CPU times: user 6min 36s, sys: 47.8 s, total: 7min 24s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history = model.fit(matrix_train, \n",
    "                    y_train, \n",
    "                    epochs=5, \n",
    "                    batch_size=128,\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.7999\n"
     ]
    }
   ],
   "source": [
    "prob_test = model.predict(matrix_test, batch_size=128)\n",
    "pred_test = np.array(prob_test) > 0.2\n",
    "print('F1 score: ', round(f1_score(y_test, pred_test, average='micro'), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity check for the first point of test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False  True False False False False False\n",
      " False False False False False False False False False False False False\n",
      "  True False False False False False False False False False False False\n",
      " False False False False False False]\n",
      "[  2.16779772e-05   2.96081835e-05   2.59374119e-05   2.03181044e-05\n",
      "   4.63070828e-05   3.29418399e-05   1.08107943e-05   2.94801848e-05\n",
      "   2.36023534e-05   3.07795017e-05   2.04586977e-05   3.34894197e-04\n",
      "   4.80589631e-04   6.14098448e-04   4.97142843e-04   2.29027099e-03\n",
      "   1.61524210e-03   6.63741317e-04   7.36741582e-04   2.43646457e-04\n",
      "   3.46810935e-04   2.30677106e-04   5.28711935e-05   4.17868869e-04\n",
      "   1.56719136e-04   1.05896986e-04   3.46616725e-04   1.24655737e-04\n",
      "   6.93555703e-05   1.52247224e-03   6.98118238e-04   5.01557544e-04\n",
      "   2.87490385e-03   1.33581867e-03   3.50361603e-04   5.44140115e-04\n",
      "   4.87102719e-04   9.21142695e-04   9.03178239e-04   3.86739004e-04\n",
      "   2.49594916e-04   1.72463793e-03   1.56418397e-03   1.94493899e-04\n",
      "   9.91988275e-03   6.81617879e-04   2.42467373e-04   3.84334970e-04\n",
      "   6.15272147e-04   4.70448693e-04   1.83196535e-04   1.11245899e-03\n",
      "   2.70893128e-04   9.87870226e-05   6.26516878e-04   3.44995921e-03\n",
      "   4.47736064e-04   1.84400263e-03   6.00834086e-04   3.72434908e-04\n",
      "   1.05017280e-04   7.84141084e-05   4.55787289e-04   5.54144150e-04\n",
      "   4.86451958e-04   4.08055173e-04   2.23830415e-04   7.89227604e-04\n",
      "   1.49240368e-04   5.00571041e-04   3.16400686e-03   3.99720993e-05\n",
      "   4.20947545e-05   5.94635676e-05   3.11379445e-05   6.32439769e-05\n",
      "   1.58096464e-05   2.42887254e-05   3.81832033e-05   2.85576207e-05\n",
      "   3.30605020e-04   1.67834762e-04   5.83400833e-04   2.77705636e-04\n",
      "   1.51747139e-04   4.93405969e-04   5.49198485e-05   2.98897387e-04\n",
      "   5.86429960e-04   1.54999616e-05   9.31549430e-01   9.89978667e-04\n",
      "   8.80413281e-04   2.22360040e-03   1.12963072e-03   3.50627524e-05\n",
      "   3.54952249e-03   8.48954311e-04   1.83767217e-04   9.37084260e-04\n",
      "   4.56139765e-04   4.70889245e-05   3.29810102e-03   2.10076733e-03\n",
      "   3.92001309e-03   3.91425285e-03   1.21663068e-03   1.69530441e-03\n",
      "   9.61084485e-01   4.03768005e-04   1.39000767e-03   7.71515653e-04\n",
      "   6.78326818e-04   3.61158542e-04   4.24836820e-04   2.70858756e-04\n",
      "   1.71983463e-03   4.62666416e-04   4.81006020e-04   1.12422334e-03\n",
      "   8.77911923e-04   1.68351500e-04   1.55689631e-04   5.13778767e-03\n",
      "   2.00752875e-05   3.12102457e-05]\n"
     ]
    }
   ],
   "source": [
    "print(y_test[0])\n",
    "print(pred_test[0])\n",
    "print(prob_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your model\n",
    "\n",
    "Finally, save your best model to the competition and return it as an `h5` file. For example like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model file should now be visible in the \"Home\" screen of the jupyter notebooks interface.  There you should be able to select it and press \"download\".\n",
    "\n",
    "## Predict for test set\n",
    "\n",
    "You will be asked to return your predictions a separate test set.  These should be returned as a matrix with one row for each test article.  Each row contains a binary prediction for each label, 1 if it's present in the image, and 0 if not. The order of the labels is the order of the label (topic) codes.\n",
    "\n",
    "An example row could like like this if your system predicts the presense of the second and fourth topic:\n",
    "\n",
    "    0 1 0 1 0 0 0 0 0 0 0 0 0 0 ...\n",
    "    \n",
    "If you have the matrix prepared in `y` (e.g., by calling `y=model.predict(x_test)`) you can use the following function to save it to a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt('results.txt', y, fmt='%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
